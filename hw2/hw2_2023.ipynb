{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBrDXMdDy-Qn"
   },
   "source": [
    "# HSE 2023: Введение в машинное обучение БИ 23/24\n",
    "\n",
    "## ДЗ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXXi5K1mf41d"
   },
   "source": [
    "# Внимание!\n",
    "Если в задании просят объяснить что-либо, то это значит, что требуется письменный ответ, который является частью задания и оценивается\n",
    "\n",
    "Мы только принимаем ipynb ноутбуки. Если вы используете Google Colab, то вам необходимо скачать ноутбук перед сдачей ДЗ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "id": "mSR-a9vVy-Qp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUjuv9Qty-Qq"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Для этого ДЗ мы будем использовать датасет треков со стримингового сервиса Spotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание данных**\n",
    "\n",
    "- **track_id:** The Spotify ID for the track\n",
    "- **artists:** The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
    "- **album_name:** The album name in which the track appears\n",
    "- **track_name:** Name of the track\n",
    "- **popularity:** The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
    "- **duration_ms:** The track length in milliseconds\n",
    "- **explicit:** Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)\n",
    "- **danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
    "- **key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1\n",
    "- **loudness:** The overall loudness of a track in decibels (dB)\n",
    "- **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0\n",
    "- **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks\n",
    "- **acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\n",
    "- **instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content\n",
    "- **liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
    "- **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
    "- **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
    "- **time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
    "- **track_genre:** The genre in which the track belongs\n",
    "\n",
    "**Целевая переменная**\n",
    "- **energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tHWSWTXDy-Qq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Count of rows, contains Nan values: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Посмотрим датасет на наличие в нем Nan\n",
    "print(f\" * Count of rows, contains Nan values: {data[data.isna().any(axis=1)].shape[0]}\\n\")\n",
    "\n",
    "# Всего одна строка, просто удалим ее\n",
    "data = data.dropna()\n",
    "\n",
    "y = data['energy']\n",
    "X = data.drop(['energy'], axis=1)\n",
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K81w8s35y-Qq"
   },
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYgEN-FMy-Qr"
   },
   "source": [
    "#### 0. [0.25 балла] Закодируйте категориальные признаки. Объясните выбранный вами метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "```\n",
    "Имена авторов и жанры я дропать не стал, так как они содержат полезную информацию. \n",
    "Большинство артистов пишут музыку в определенном узнаваемом стиле, потому, увидев в строке Rammstein или Depeche mode, \n",
    "можно предсказать энергетику песни. Ну с жанрами тоже самое. Я решил закодировать эти признаки с помощью target encoding, потому что \n",
    "они не является упорядоченными и содержат большое количество значений, что не очень хочется превращать в дополнительные колонки\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-IrSlQaWy-Qr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Categorical: ['artists', 'album_name', 'track_name', 'track_genre']\n",
      " * Unique values: \n",
      "\tArtists - 31437 \n",
      "\tGenres - 114\n"
     ]
    }
   ],
   "source": [
    "# Найдем все категориальные признаки\n",
    "categorical = list(X.dtypes[X.dtypes == \"object\"].index)\n",
    "print(f\" * Categorical: {categorical}\")\n",
    "\n",
    "# Я дропнул признаки с названиями, потому что они не несут никакой прикладной информации об энергичности трека.\n",
    "X = X.drop([\"album_name\", \"track_name\"], axis=1)\n",
    "categorical.remove(\"album_name\")\n",
    "categorical.remove(\"track_name\")\n",
    "\n",
    "print(f\" * Unique values: \\n\\tArtists - {X['artists'].nunique()} \\n\\tGenres - {X['track_genre'].nunique()}\")\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "target_encoder = TargetEncoder()\n",
    "X_encoded = target_encoder.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dVwP45Gy-Qr"
   },
   "source": [
    "#### 1. [0.25 балла] Разбейте данные на train и test с пропорцией 75:25 и random_state=7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U7z8TIh5y-Qs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7daIQRfKy-Qs",
    "tags": []
   },
   "source": [
    "#### 2. [0.75 балла] Обучите модели на train'е, исключив категориальные признаки, используя библиотеку StatsModels и примените ее к test'у; используйте $RMSE$ и $R ^ 2$ в качестве метрики качества. Попробуйте также применить реализации линейной регрессии из sklearn:\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.03$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.05$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.4$\n",
    "\n",
    "Не забывайте скейлить данные с помощью StandardScaler перед обучением моделей! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, обучающая модели\n",
    "def train_on_models (X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Масштабируем признаки\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(data=scaler.transform(X_test), columns=X_test.columns)\n",
    "    y_train = list(y_train)\n",
    "    y_test = list(y_test)\n",
    "\n",
    "    X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "    X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "    \n",
    "    models = [ \n",
    "               (\"LinearRegressionModel\", sm.OLS(y_train, X_train_scaled).fit(), # statsmodels\n",
    "                LinearRegression().fit(X_train_scaled, y_train)), # sklearn\n",
    "               (\"RidgeModel\\t\", sm.OLS(y_train, X_train_scaled).fit_regularized(method='elastic_net', # statsmodels\n",
    "                                                                                    L1_wt=0.0000001, alpha=0.03, refit=True),\n",
    "                Ridge(alpha=0.03).fit(X_train_scaled, y_train)), # sklearn\n",
    "               (\"LassoModel\\t\", sm.OLS(y_train, X_train_scaled).fit_regularized(method='elastic_net', # statsmodels\n",
    "                                                                                    L1_wt=1, alpha=0.05, refit=True),\n",
    "                Lasso(alpha=0.05).fit(X_train_scaled, y_train)), # sklearn\n",
    "               (\"ElasricNetModel\\t\", sm.OLS(y_train, X_train_scaled).fit_regularized(method='elastic_net', # statsmodels\n",
    "                                                                                    L1_wt=0.4, alpha=0.01, refit=True),\n",
    "                ElasticNet(alpha=0.01, l1_ratio=0.4).fit(X_train_scaled, y_train)) # sklearn\n",
    "             ]\n",
    "    print(3*'\\t' + \"StatsModels:\" + '\\t' + \"Sklearn:\")\n",
    "    \n",
    "    for model in models: \n",
    "        y_pred_statsmodels = model[1].predict(X_test_scaled)\n",
    "        y_pred_sklean = model[2].predict(X_test_scaled)\n",
    "        \n",
    "        print(f\"{model[0]}\", end='')\n",
    "        print(\"\\tRMSE = %.4f\" % mean_squared_error(y_test, y_pred_statsmodels, squared=False), end='')\n",
    "        print(\"\\tRMSE = %.4f\" % mean_squared_error(y_test, y_pred_sklean, squared=False))\n",
    "\n",
    "    for model in models:\n",
    "        y_pred_statsmodels = model[1].predict(X_test_scaled)\n",
    "        y_pred_sklean = model[2].predict(X_test_scaled)\n",
    "        \n",
    "        print(f\"{model[0]}\", end='')\n",
    "        print(\"\\tR^2 = %.4f\" % r2_score(y_test, y_pred_statsmodels), end='')\n",
    "        print(\"\\tR^2 = %.4f\" % r2_score(y_test, y_pred_sklean))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Bkbr5iFCy-Qs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tStatsModels:\tSklearn:\n",
      "LinearRegressionModel\tRMSE = 0.1215\tRMSE = 0.1215\n",
      "RidgeModel\t\tRMSE = 0.1215\tRMSE = 0.1215\n",
      "LassoModel\t\tRMSE = 0.1367\tRMSE = 0.1473\n",
      "ElasricNetModel\t\tRMSE = 0.1218\tRMSE = 0.1225\n",
      "LinearRegressionModel\tR^2 = 0.7659\tR^2 = 0.7659\n",
      "RidgeModel\t\tR^2 = 0.7659\tR^2 = 0.7659\n",
      "LassoModel\t\tR^2 = 0.7041\tR^2 = 0.6561\n",
      "ElasricNetModel\t\tR^2 = 0.7648\tR^2 = 0.7621\n"
     ]
    }
   ],
   "source": [
    "# Получим все некатегориальные признаки \n",
    "X_train_wo_cat = X_train.drop(categorical, axis=1)\n",
    "X_test_wo_cat = X_test.drop(categorical, axis=1)\n",
    "\n",
    "models_wo_cat = train_on_models(X_train_wo_cat, X_test_wo_cat, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [0.25 балла] Повторите шаги из предыдущего пункта, добавив категориальные признаки. Прокомментируйте изменения значений метрик качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "``` \n",
    "Метрика RMSE стала меньше, а R^2 больше, что свидетельствует о повышении качества обученной модели.\n",
    "В целом ожидаемо, ведь, как я раньше писал, данные категориальные признаки содержат полезную информацию\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tStatsModels:\tSklearn:\n",
      "LinearRegressionModel\tRMSE = 0.1123\tRMSE = 0.1123\n",
      "RidgeModel\t\tRMSE = 0.1123\tRMSE = 0.1123\n",
      "LassoModel\t\tRMSE = 0.1258\tRMSE = 0.1374\n",
      "ElasricNetModel\t\tRMSE = 0.1156\tRMSE = 0.1133\n",
      "LinearRegressionModel\tR^2 = 0.8002\tR^2 = 0.8002\n",
      "RidgeModel\t\tR^2 = 0.8002\tR^2 = 0.8002\n",
      "LassoModel\t\tR^2 = 0.7493\tR^2 = 0.7009\n",
      "ElasricNetModel\t\tR^2 = 0.7881\tR^2 = 0.7966\n"
     ]
    }
   ],
   "source": [
    "models = train_on_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69JOftKRy-Qt"
   },
   "source": [
    "#### 4. [1 балл] Исследуйте значения параметров полученных моделей и проверьте какие веса получились нулевыми. Прокомментируйте значимость коэффициентов, обшую значимость модели и остальные факторы из результирующей таблицы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np1biYQ7y-Qt"
   },
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "```\n",
    "Как мы можем наблюдать, абсолютно все параметры, кроме Key имеют pvalue равное 0, что говорит нам о \n",
    "статистической важности этих параметров для модели, кроме того, вышло так, что я выбрал удачные категориальные признаки\n",
    "и выбрал удачный вариант их кодирования\n",
    "\n",
    "Значение коэффициентов при параметрах указывают на направление влияния на модель. То есть, отрицательные значения \n",
    "параметров popularity, explicit, danceability, mode, acousticness говорят о отрицательном влиянии на результирующую\n",
    "модель. Но вот что странно, я попробовал обучить модель, выбросив эти параметры, \n",
    "и метрика R-squared приняла значение 0.756, что меньше текущего, что говорит о ухудшемся качестве модели (на изображении в след ячейке)\n",
    "Как я понял, дело в том, что Key стало больше влиять на модель, но ее коффициент остался очень маленьким\n",
    "\n",
    "Можем заметить, что подобная тенденция сохраняется для всех моделей\n",
    "\n",
    "Как я понял, nan значения p появляются когда вероятность равна 0 или 1, в нашем случае p = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.802</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-131902.0414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 06:20</td>        <td>BIC:</td>         <td>-131742.9850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>65968.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>16</td>           <td>F-statistic:</td>       <td>2.169e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85482</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.802</td>            <td>Scale:</td>          <td>0.012515</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1677.8057</td> <td>0.0000</td> <td>0.6412</td>  <td>0.6427</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0508</td>   <td>0.0005</td>   <td>94.6209</td>  <td>0.0000</td> <td>0.0498</td>  <td>0.0519</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>-0.0029</td>  <td>0.0004</td>   <td>-7.4317</td>  <td>0.0000</td> <td>-0.0036</td> <td>-0.0021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0017</td>   <td>0.0004</td>   <td>4.2190</td>   <td>0.0000</td> <td>0.0009</td>  <td>0.0024</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>-0.0027</td>  <td>0.0004</td>   <td>-6.4926</td>  <td>0.0000</td> <td>-0.0035</td> <td>-0.0019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>-0.0269</td>  <td>0.0005</td>  <td>-57.4729</td>  <td>0.0000</td> <td>-0.0279</td> <td>-0.0260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0004</td>   <td>0.0004</td>   <td>1.1541</td>   <td>0.2485</td> <td>-0.0003</td> <td>0.0012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1113</td>   <td>0.0006</td>  <td>191.7855</td>  <td>0.0000</td> <td>0.1102</td>  <td>0.1125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>-0.0026</td>  <td>0.0004</td>   <td>-6.6334</td>  <td>0.0000</td> <td>-0.0034</td> <td>-0.0018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0217</td>   <td>0.0004</td>   <td>51.8706</td>  <td>0.0000</td> <td>0.0209</td>  <td>0.0225</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0744</td>  <td>0.0006</td>  <td>-132.5116</td> <td>0.0000</td> <td>-0.0755</td> <td>-0.0733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0334</td>   <td>0.0005</td>   <td>73.8899</td>  <td>0.0000</td> <td>0.0325</td>  <td>0.0343</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0212</td>   <td>0.0004</td>   <td>52.5667</td>  <td>0.0000</td> <td>0.0204</td>  <td>0.0219</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0376</td>   <td>0.0005</td>   <td>80.2445</td>  <td>0.0000</td> <td>0.0366</td>  <td>0.0385</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0056</td>   <td>0.0004</td>   <td>13.8626</td>  <td>0.0000</td> <td>0.0048</td>  <td>0.0063</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0050</td>   <td>0.0004</td>   <td>12.5746</td>  <td>0.0000</td> <td>0.0042</td>  <td>0.0058</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0260</td>   <td>0.0006</td>   <td>46.0216</td>  <td>0.0000</td> <td>0.0249</td>  <td>0.0271</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>5424.939</td>  <td>Durbin-Watson:</td>     <td>1.998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>23851.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.133</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>5.574</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.802         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -131902.0414  \\\\\n",
       "Date:               & 2023-10-23 06:20 & BIC:                & -131742.9850  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 65968.        \\\\\n",
       "Df Model:           & 16               & F-statistic:        & 2.169e+04     \\\\\n",
       "Df Residuals:       & 85482            & Prob (F-statistic): & 0.00          \\\\\n",
       "R-squared:          & 0.802            & Scale:              & 0.012515      \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1677.8057 &      0.0000 &  0.6412 &  0.6427  \\\\\n",
       "artists          &  0.0508 &   0.0005 &   94.6209 &      0.0000 &  0.0498 &  0.0519  \\\\\n",
       "popularity       & -0.0029 &   0.0004 &   -7.4317 &      0.0000 & -0.0036 & -0.0021  \\\\\n",
       "duration\\_ms     &  0.0017 &   0.0004 &    4.2190 &      0.0000 &  0.0009 &  0.0024  \\\\\n",
       "explicit         & -0.0027 &   0.0004 &   -6.4926 &      0.0000 & -0.0035 & -0.0019  \\\\\n",
       "danceability     & -0.0269 &   0.0005 &  -57.4729 &      0.0000 & -0.0279 & -0.0260  \\\\\n",
       "key              &  0.0004 &   0.0004 &    1.1541 &      0.2485 & -0.0003 &  0.0012  \\\\\n",
       "loudness         &  0.1113 &   0.0006 &  191.7855 &      0.0000 &  0.1102 &  0.1125  \\\\\n",
       "mode             & -0.0026 &   0.0004 &   -6.6334 &      0.0000 & -0.0034 & -0.0018  \\\\\n",
       "speechiness      &  0.0217 &   0.0004 &   51.8706 &      0.0000 &  0.0209 &  0.0225  \\\\\n",
       "acousticness     & -0.0744 &   0.0006 & -132.5116 &      0.0000 & -0.0755 & -0.0733  \\\\\n",
       "instrumentalness &  0.0334 &   0.0005 &   73.8899 &      0.0000 &  0.0325 &  0.0343  \\\\\n",
       "liveness         &  0.0212 &   0.0004 &   52.5667 &      0.0000 &  0.0204 &  0.0219  \\\\\n",
       "valence          &  0.0376 &   0.0005 &   80.2445 &      0.0000 &  0.0366 &  0.0385  \\\\\n",
       "tempo            &  0.0056 &   0.0004 &   13.8626 &      0.0000 &  0.0048 &  0.0063  \\\\\n",
       "time\\_signature  &  0.0050 &   0.0004 &   12.5746 &      0.0000 &  0.0042 &  0.0058  \\\\\n",
       "track\\_genre     &  0.0260 &   0.0006 &   46.0216 &      0.0000 &  0.0249 &  0.0271  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 5424.939 & Durbin-Watson:    & 1.998      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 23851.079  \\\\\n",
       "Skew:          & 0.133    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 5.574    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.802       \n",
       "Dependent Variable: y                AIC:                -131902.0414\n",
       "Date:               2023-10-23 06:20 BIC:                -131742.9850\n",
       "No. Observations:   85499            Log-Likelihood:     65968.      \n",
       "Df Model:           16               F-statistic:        2.169e+04   \n",
       "Df Residuals:       85482            Prob (F-statistic): 0.00        \n",
       "R-squared:          0.802            Scale:              0.012515    \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1677.8057 0.0000  0.6412  0.6427\n",
       "artists              0.0508   0.0005   94.6209 0.0000  0.0498  0.0519\n",
       "popularity          -0.0029   0.0004   -7.4317 0.0000 -0.0036 -0.0021\n",
       "duration_ms          0.0017   0.0004    4.2190 0.0000  0.0009  0.0024\n",
       "explicit            -0.0027   0.0004   -6.4926 0.0000 -0.0035 -0.0019\n",
       "danceability        -0.0269   0.0005  -57.4729 0.0000 -0.0279 -0.0260\n",
       "key                  0.0004   0.0004    1.1541 0.2485 -0.0003  0.0012\n",
       "loudness             0.1113   0.0006  191.7855 0.0000  0.1102  0.1125\n",
       "mode                -0.0026   0.0004   -6.6334 0.0000 -0.0034 -0.0018\n",
       "speechiness          0.0217   0.0004   51.8706 0.0000  0.0209  0.0225\n",
       "acousticness        -0.0744   0.0006 -132.5116 0.0000 -0.0755 -0.0733\n",
       "instrumentalness     0.0334   0.0005   73.8899 0.0000  0.0325  0.0343\n",
       "liveness             0.0212   0.0004   52.5667 0.0000  0.0204  0.0219\n",
       "valence              0.0376   0.0005   80.2445 0.0000  0.0366  0.0385\n",
       "tempo                0.0056   0.0004   13.8626 0.0000  0.0048  0.0063\n",
       "time_signature       0.0050   0.0004   12.5746 0.0000  0.0042  0.0058\n",
       "track_genre          0.0260   0.0006   46.0216 0.0000  0.0249  0.0271\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             5424.939       Durbin-Watson:          1.998    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       23851.079\n",
       "Skew:                0.133          Prob(JB):               0.000    \n",
       "Kurtosis:            5.574          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear regression \n",
    "models[0][1].summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"results_wo_params.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.802</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-131900.0414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 06:20</td>        <td>BIC:</td>         <td>-131731.6287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>65968.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>17</td>           <td>F-statistic:</td>       <td>2.042e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85482</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>          <td>elastic_net</td>         <td>Scale:</td>          <td>0.012515</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.802</td>               <td></td>                 <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1677.8057</td> <td>0.0000</td> <td>0.6412</td>  <td>0.6427</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0508</td>   <td>0.0005</td>   <td>94.6209</td>  <td>0.0000</td> <td>0.0498</td>  <td>0.0519</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>-0.0029</td>  <td>0.0004</td>   <td>-7.4317</td>  <td>0.0000</td> <td>-0.0036</td> <td>-0.0021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0017</td>   <td>0.0004</td>   <td>4.2190</td>   <td>0.0000</td> <td>0.0009</td>  <td>0.0024</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>-0.0027</td>  <td>0.0004</td>   <td>-6.4926</td>  <td>0.0000</td> <td>-0.0035</td> <td>-0.0019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>-0.0269</td>  <td>0.0005</td>  <td>-57.4729</td>  <td>0.0000</td> <td>-0.0279</td> <td>-0.0260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0004</td>   <td>0.0004</td>   <td>1.1541</td>   <td>0.2485</td> <td>-0.0003</td> <td>0.0012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1113</td>   <td>0.0006</td>  <td>191.7855</td>  <td>0.0000</td> <td>0.1102</td>  <td>0.1125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>-0.0026</td>  <td>0.0004</td>   <td>-6.6334</td>  <td>0.0000</td> <td>-0.0034</td> <td>-0.0018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0217</td>   <td>0.0004</td>   <td>51.8706</td>  <td>0.0000</td> <td>0.0209</td>  <td>0.0225</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0744</td>  <td>0.0006</td>  <td>-132.5116</td> <td>0.0000</td> <td>-0.0755</td> <td>-0.0733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0334</td>   <td>0.0005</td>   <td>73.8899</td>  <td>0.0000</td> <td>0.0325</td>  <td>0.0343</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0212</td>   <td>0.0004</td>   <td>52.5667</td>  <td>0.0000</td> <td>0.0204</td>  <td>0.0219</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0376</td>   <td>0.0005</td>   <td>80.2445</td>  <td>0.0000</td> <td>0.0366</td>  <td>0.0385</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0056</td>   <td>0.0004</td>   <td>13.8626</td>  <td>0.0000</td> <td>0.0048</td>  <td>0.0063</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0050</td>   <td>0.0004</td>   <td>12.5746</td>  <td>0.0000</td> <td>0.0042</td>  <td>0.0058</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0260</td>   <td>0.0006</td>   <td>46.0216</td>  <td>0.0000</td> <td>0.0249</td>  <td>0.0271</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>5424.939</td>  <td>Durbin-Watson:</td>     <td>1.998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>23851.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.133</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>5.574</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.802         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -131900.0414  \\\\\n",
       "Date:               & 2023-10-23 06:20 & BIC:                & -131731.6287  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 65968.        \\\\\n",
       "Df Model:           & 17               & F-statistic:        & 2.042e+04     \\\\\n",
       "Df Residuals:       & 85482            & Prob (F-statistic): & 0.00          \\\\\n",
       "Method:             & elastic\\_net     & Scale:              & 0.012515      \\\\\n",
       "R-squared:          & 0.802            &                     &               \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1677.8057 &      0.0000 &  0.6412 &  0.6427  \\\\\n",
       "artists          &  0.0508 &   0.0005 &   94.6209 &      0.0000 &  0.0498 &  0.0519  \\\\\n",
       "popularity       & -0.0029 &   0.0004 &   -7.4317 &      0.0000 & -0.0036 & -0.0021  \\\\\n",
       "duration\\_ms     &  0.0017 &   0.0004 &    4.2190 &      0.0000 &  0.0009 &  0.0024  \\\\\n",
       "explicit         & -0.0027 &   0.0004 &   -6.4926 &      0.0000 & -0.0035 & -0.0019  \\\\\n",
       "danceability     & -0.0269 &   0.0005 &  -57.4729 &      0.0000 & -0.0279 & -0.0260  \\\\\n",
       "key              &  0.0004 &   0.0004 &    1.1541 &      0.2485 & -0.0003 &  0.0012  \\\\\n",
       "loudness         &  0.1113 &   0.0006 &  191.7855 &      0.0000 &  0.1102 &  0.1125  \\\\\n",
       "mode             & -0.0026 &   0.0004 &   -6.6334 &      0.0000 & -0.0034 & -0.0018  \\\\\n",
       "speechiness      &  0.0217 &   0.0004 &   51.8706 &      0.0000 &  0.0209 &  0.0225  \\\\\n",
       "acousticness     & -0.0744 &   0.0006 & -132.5116 &      0.0000 & -0.0755 & -0.0733  \\\\\n",
       "instrumentalness &  0.0334 &   0.0005 &   73.8899 &      0.0000 &  0.0325 &  0.0343  \\\\\n",
       "liveness         &  0.0212 &   0.0004 &   52.5667 &      0.0000 &  0.0204 &  0.0219  \\\\\n",
       "valence          &  0.0376 &   0.0005 &   80.2445 &      0.0000 &  0.0366 &  0.0385  \\\\\n",
       "tempo            &  0.0056 &   0.0004 &   13.8626 &      0.0000 &  0.0048 &  0.0063  \\\\\n",
       "time\\_signature  &  0.0050 &   0.0004 &   12.5746 &      0.0000 &  0.0042 &  0.0058  \\\\\n",
       "track\\_genre     &  0.0260 &   0.0006 &   46.0216 &      0.0000 &  0.0249 &  0.0271  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 5424.939 & Durbin-Watson:    & 1.998      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 23851.079  \\\\\n",
       "Skew:          & 0.133    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 5.574    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.802       \n",
       "Dependent Variable: y                AIC:                -131900.0414\n",
       "Date:               2023-10-23 06:20 BIC:                -131731.6287\n",
       "No. Observations:   85499            Log-Likelihood:     65968.      \n",
       "Df Model:           17               F-statistic:        2.042e+04   \n",
       "Df Residuals:       85482            Prob (F-statistic): 0.00        \n",
       "Method:             elastic_net      Scale:              0.012515    \n",
       "R-squared:          0.802                                            \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1677.8057 0.0000  0.6412  0.6427\n",
       "artists              0.0508   0.0005   94.6209 0.0000  0.0498  0.0519\n",
       "popularity          -0.0029   0.0004   -7.4317 0.0000 -0.0036 -0.0021\n",
       "duration_ms          0.0017   0.0004    4.2190 0.0000  0.0009  0.0024\n",
       "explicit            -0.0027   0.0004   -6.4926 0.0000 -0.0035 -0.0019\n",
       "danceability        -0.0269   0.0005  -57.4729 0.0000 -0.0279 -0.0260\n",
       "key                  0.0004   0.0004    1.1541 0.2485 -0.0003  0.0012\n",
       "loudness             0.1113   0.0006  191.7855 0.0000  0.1102  0.1125\n",
       "mode                -0.0026   0.0004   -6.6334 0.0000 -0.0034 -0.0018\n",
       "speechiness          0.0217   0.0004   51.8706 0.0000  0.0209  0.0225\n",
       "acousticness        -0.0744   0.0006 -132.5116 0.0000 -0.0755 -0.0733\n",
       "instrumentalness     0.0334   0.0005   73.8899 0.0000  0.0325  0.0343\n",
       "liveness             0.0212   0.0004   52.5667 0.0000  0.0204  0.0219\n",
       "valence              0.0376   0.0005   80.2445 0.0000  0.0366  0.0385\n",
       "tempo                0.0056   0.0004   13.8626 0.0000  0.0048  0.0063\n",
       "time_signature       0.0050   0.0004   12.5746 0.0000  0.0042  0.0058\n",
       "track_genre          0.0260   0.0006   46.0216 0.0000  0.0249  0.0271\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             5424.939       Durbin-Watson:          1.998    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       23851.079\n",
       "Skew:                0.133          Prob(JB):               0.000    \n",
       "Kurtosis:            5.574          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge\n",
    "models[1][1].summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.752</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-112500.0136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 06:20</td>        <td>BIC:</td>         <td>-112453.2323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>56255.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>4</td>           <td>F-statistic:</td>       <td>6.480e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85495</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>          <td>elastic_net</td>         <td>Scale:</td>          <td>0.015705</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.752</td>               <td></td>                 <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1497.7425</td> <td>0.0000</td> <td>0.6411</td>  <td>0.6428</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0709</td>   <td>0.0006</td>  <td>127.8784</td>  <td>0.0000</td> <td>0.0698</td>  <td>0.0720</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1044</td>   <td>0.0006</td>  <td>186.7080</td>  <td>0.0000</td> <td>0.1033</td>  <td>0.1055</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0821</td>  <td>0.0006</td>  <td>-143.5493</td> <td>0.0000</td> <td>-0.0833</td> <td>-0.0810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>4212.831</td>  <td>Durbin-Watson:</td>     <td>1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>11879.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.239</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>4.762</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.752         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -112500.0136  \\\\\n",
       "Date:               & 2023-10-23 06:20 & BIC:                & -112453.2323  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 56255.        \\\\\n",
       "Df Model:           & 4                & F-statistic:        & 6.480e+04     \\\\\n",
       "Df Residuals:       & 85495            & Prob (F-statistic): & 0.00          \\\\\n",
       "Method:             & elastic\\_net     & Scale:              & 0.015705      \\\\\n",
       "R-squared:          & 0.752            &                     &               \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1497.7425 &      0.0000 &  0.6411 &  0.6428  \\\\\n",
       "artists          &  0.0709 &   0.0006 &  127.8784 &      0.0000 &  0.0698 &  0.0720  \\\\\n",
       "popularity       &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "duration\\_ms     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "explicit         &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "danceability     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "key              &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "loudness         &  0.1044 &   0.0006 &  186.7080 &      0.0000 &  0.1033 &  0.1055  \\\\\n",
       "mode             &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "speechiness      &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "acousticness     & -0.0821 &   0.0006 & -143.5493 &      0.0000 & -0.0833 & -0.0810  \\\\\n",
       "instrumentalness &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "liveness         &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "valence          &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "tempo            &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "time\\_signature  &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "track\\_genre     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 4212.831 & Durbin-Watson:    & 1.997      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 11879.118  \\\\\n",
       "Skew:          & 0.239    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 4.762    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.752       \n",
       "Dependent Variable: y                AIC:                -112500.0136\n",
       "Date:               2023-10-23 06:20 BIC:                -112453.2323\n",
       "No. Observations:   85499            Log-Likelihood:     56255.      \n",
       "Df Model:           4                F-statistic:        6.480e+04   \n",
       "Df Residuals:       85495            Prob (F-statistic): 0.00        \n",
       "Method:             elastic_net      Scale:              0.015705    \n",
       "R-squared:          0.752                                            \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1497.7425 0.0000  0.6411  0.6428\n",
       "artists              0.0709   0.0006  127.8784 0.0000  0.0698  0.0720\n",
       "popularity           0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "duration_ms          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "explicit             0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "danceability         0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "key                  0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "loudness             0.1044   0.0006  186.7080 0.0000  0.1033  0.1055\n",
       "mode                 0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "speechiness          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "acousticness        -0.0821   0.0006 -143.5493 0.0000 -0.0833 -0.0810\n",
       "instrumentalness     0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "liveness             0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "valence              0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "tempo                0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "time_signature       0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "track_genre          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             4212.831       Durbin-Watson:          1.997    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       11879.118\n",
       "Skew:                0.239          Prob(JB):               0.000    \n",
       "Kurtosis:            4.762          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso\n",
    "models[2][1].summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.790</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-126585.0528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 06:20</td>        <td>BIC:</td>         <td>-126482.1339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>63304.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>10</td>           <td>F-statistic:</td>       <td>3.210e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85489</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>          <td>elastic_net</td>         <td>Scale:</td>          <td>0.013319</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.790</td>               <td></td>                 <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1626.3914</td> <td>0.0000</td> <td>0.6411</td>  <td>0.6427</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0626</td>   <td>0.0005</td>  <td>121.0446</td>  <td>0.0000</td> <td>0.0615</td>  <td>0.0636</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>-0.0044</td>  <td>0.0004</td>  <td>-10.9294</td>  <td>0.0000</td> <td>-0.0051</td> <td>-0.0036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1124</td>   <td>0.0006</td>  <td>192.9112</td>  <td>0.0000</td> <td>0.1113</td>  <td>0.1135</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0191</td>   <td>0.0004</td>   <td>46.9316</td>  <td>0.0000</td> <td>0.0183</td>  <td>0.0199</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0804</td>  <td>0.0005</td>  <td>-148.7908</td> <td>0.0000</td> <td>-0.0815</td> <td>-0.0794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0338</td>   <td>0.0005</td>   <td>72.9604</td>  <td>0.0000</td> <td>0.0329</td>  <td>0.0347</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0261</td>   <td>0.0004</td>   <td>64.2627</td>  <td>0.0000</td> <td>0.0253</td>  <td>0.0269</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0250</td>   <td>0.0004</td>   <td>58.8128</td>  <td>0.0000</td> <td>0.0241</td>  <td>0.0258</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0101</td>   <td>0.0004</td>   <td>24.7592</td>  <td>0.0000</td> <td>0.0093</td>  <td>0.0109</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>4400.795</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>16699.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.085</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>5.158</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.790         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -126585.0528  \\\\\n",
       "Date:               & 2023-10-23 06:20 & BIC:                & -126482.1339  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 63304.        \\\\\n",
       "Df Model:           & 10               & F-statistic:        & 3.210e+04     \\\\\n",
       "Df Residuals:       & 85489            & Prob (F-statistic): & 0.00          \\\\\n",
       "Method:             & elastic\\_net     & Scale:              & 0.013319      \\\\\n",
       "R-squared:          & 0.790            &                     &               \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1626.3914 &      0.0000 &  0.6411 &  0.6427  \\\\\n",
       "artists          &  0.0626 &   0.0005 &  121.0446 &      0.0000 &  0.0615 &  0.0636  \\\\\n",
       "popularity       & -0.0044 &   0.0004 &  -10.9294 &      0.0000 & -0.0051 & -0.0036  \\\\\n",
       "duration\\_ms     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "explicit         &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "danceability     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "key              &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "loudness         &  0.1124 &   0.0006 &  192.9112 &      0.0000 &  0.1113 &  0.1135  \\\\\n",
       "mode             &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "speechiness      &  0.0191 &   0.0004 &   46.9316 &      0.0000 &  0.0183 &  0.0199  \\\\\n",
       "acousticness     & -0.0804 &   0.0005 & -148.7908 &      0.0000 & -0.0815 & -0.0794  \\\\\n",
       "instrumentalness &  0.0338 &   0.0005 &   72.9604 &      0.0000 &  0.0329 &  0.0347  \\\\\n",
       "liveness         &  0.0261 &   0.0004 &   64.2627 &      0.0000 &  0.0253 &  0.0269  \\\\\n",
       "valence          &  0.0250 &   0.0004 &   58.8128 &      0.0000 &  0.0241 &  0.0258  \\\\\n",
       "tempo            &  0.0101 &   0.0004 &   24.7592 &      0.0000 &  0.0093 &  0.0109  \\\\\n",
       "time\\_signature  &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "track\\_genre     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 4400.795 & Durbin-Watson:    & 1.996      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 16699.588  \\\\\n",
       "Skew:          & 0.085    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 5.158    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.790       \n",
       "Dependent Variable: y                AIC:                -126585.0528\n",
       "Date:               2023-10-23 06:20 BIC:                -126482.1339\n",
       "No. Observations:   85499            Log-Likelihood:     63304.      \n",
       "Df Model:           10               F-statistic:        3.210e+04   \n",
       "Df Residuals:       85489            Prob (F-statistic): 0.00        \n",
       "Method:             elastic_net      Scale:              0.013319    \n",
       "R-squared:          0.790                                            \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1626.3914 0.0000  0.6411  0.6427\n",
       "artists              0.0626   0.0005  121.0446 0.0000  0.0615  0.0636\n",
       "popularity          -0.0044   0.0004  -10.9294 0.0000 -0.0051 -0.0036\n",
       "duration_ms          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "explicit             0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "danceability         0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "key                  0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "loudness             0.1124   0.0006  192.9112 0.0000  0.1113  0.1135\n",
       "mode                 0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "speechiness          0.0191   0.0004   46.9316 0.0000  0.0183  0.0199\n",
       "acousticness        -0.0804   0.0005 -148.7908 0.0000 -0.0815 -0.0794\n",
       "instrumentalness     0.0338   0.0005   72.9604 0.0000  0.0329  0.0347\n",
       "liveness             0.0261   0.0004   64.2627 0.0000  0.0253  0.0269\n",
       "valence              0.0250   0.0004   58.8128 0.0000  0.0241  0.0258\n",
       "tempo                0.0101   0.0004   24.7592 0.0000  0.0093  0.0109\n",
       "time_signature       0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "track_genre          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             4400.795       Durbin-Watson:          1.996    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       16699.588\n",
       "Skew:                0.085          Prob(JB):               0.000    \n",
       "Kurtosis:            5.158          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ElasricNet\n",
    "models[3][1].summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLcvGlUZy-Qt"
   },
   "source": [
    "#### 5. [1 балл] Реализуйте один из алгоритмов отбора признаков (Elimination by P-value, Forward elimination, Backward elimination), сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "```\n",
    "Для проверки алгоритма Backward elimination я закинул в функцию исходные данные, со всеми признаками, закодированными target encode,\n",
    "чтобы проверить качество обученной модели, если дать модели самой решить какие признаки нужны, а какие, напротив, мешают\n",
    "\n",
    "Так же, для проверки работоспособности алгоритма я закинул в него максимальное значение p = 0, хотя на практике я бы выбрал p = 0.05\n",
    "\n",
    "Пусть метрика r^2 adj осталась неизменной, зато мы избавились от переменных, которые не влияли на результат,\n",
    "а значит облегчили модель\n",
    "\n",
    "Но, по сути, если бы у нас были явно плохие параметры, имеющие очень большое p, то удаление данного параметра явно оказало бы на модель положительный эффект\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination (X, y, max_p_value):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns))\n",
    "    y_train = list(y)\n",
    "    \n",
    "    model = sm.OLS(y_train, X_train_scaled).fit()\n",
    "    print(f\"Before - r^2 adj = {model.rsquared_adj}\\n\")\n",
    "    print(model.pvalues)\n",
    "    \n",
    "    while (max(model.pvalues) > max_p_value):\n",
    "        params_w_p_greater_than_given = model.pvalues[model.pvalues == max(model.pvalues)].keys()\n",
    "        X_train_scaled = X_train_scaled.drop(params_w_p_greater_than_given, axis=1)\n",
    "\n",
    "        model = sm.OLS(y_train, X_train_scaled).fit()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tmp = data['energy']\n",
    "X_tmp = data.drop(['energy'], axis=1)\n",
    "target_encoder = TargetEncoder()\n",
    "X_encoded_tmp = target_encoder.fit_transform(X_tmp, y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TnrbRbkwy-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - r^2 adj = 0.8534447110006806\n",
      "\n",
      "const                0.000000e+00\n",
      "artists             1.915545e-263\n",
      "album_name           0.000000e+00\n",
      "track_name           0.000000e+00\n",
      "popularity           8.719158e-04\n",
      "duration_ms          5.288067e-03\n",
      "explicit             1.030235e-13\n",
      "danceability         0.000000e+00\n",
      "key                  2.268750e-04\n",
      "loudness             0.000000e+00\n",
      "mode                 2.459871e-02\n",
      "speechiness          0.000000e+00\n",
      "acousticness         0.000000e+00\n",
      "instrumentalness     0.000000e+00\n",
      "liveness             0.000000e+00\n",
      "valence              0.000000e+00\n",
      "tempo                9.145288e-36\n",
      "time_signature       4.227223e-34\n",
      "track_genre          0.000000e+00\n",
      "dtype: float64\n",
      "\n",
      "After - r^2 adj = 0.8513892068505113\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const               0.0\n",
       "album_name          0.0\n",
       "track_name          0.0\n",
       "danceability        0.0\n",
       "loudness            0.0\n",
       "speechiness         0.0\n",
       "acousticness        0.0\n",
       "instrumentalness    0.0\n",
       "liveness            0.0\n",
       "valence             0.0\n",
       "track_genre         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue = 0\n",
    "# Тут для проверки реализации я задал значение p равное 0, но по факту оптимальным будет 0.05\n",
    "\n",
    "result_model = backward_elimination(X_encoded_tmp, y_tmp, pvalue)\n",
    "\n",
    "print(f\"\\nAfter - r^2 adj = {result_model.rsquared_adj}\\n\")\n",
    "result_model.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df0eQLdNy-Qt"
   },
   "source": [
    "#### 6. [1 балл] Найдите лучший (по RMSE) $\\alpha$ для регрессиии Lasso, используя кросс-валидацию на 5 фолдов. Вы должны выбрать значение из промежутка $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JPoT3YHqy-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 0.0001\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_encoded_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X_encoded), columns=X_encoded.columns))\n",
    "\n",
    "alphas = np.logspace(-4, 3, num=8)\n",
    "searcher = GridSearchCV(Lasso(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_encoded_scaled, y) # Данные, не разделенные на train/test\n",
    "best_alpha = searcher.best_params_[\"alpha\"]\n",
    "print(\"Best alpha = %.4f\" % best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1PKinJUy-Qt"
   },
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "#### 7. [3.5 балла] Имплементируйте  Ridge регрессию для MSE loss, обученную на градиентом спуске.\n",
    "\n",
    "\n",
    "Все вычисления должны быть векторизованы, а циклы Python можно использовать только для итераций градиентного спуска. В качестве критерия остановки необходимо использовать (одновременно):\n",
    "\n",
    "* проверка абсолютной нормы разницы весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, заданного параметром `tolerance`);\n",
    "\n",
    "* достижение максимального количества итераций (например, 10000, заданного параметром `max_iter`).\n",
    "\n",
    "Вам необходимо выполнить:\n",
    "\n",
    "* Полный градиентный спуск:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Стохастический градиентный спуск:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ является оценкой градиента по набору объектов, выбранных случайным образом.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "Чтобы убедиться, что процесс оптимизации действительно выполняется, мы будем использовать атрибут класса `loss_history`. После вызова метода fit он должен содержать значения функции потерь для всех итераций, начиная с первой (до первого шага по антиградиенту).\n",
    "\n",
    "\n",
    "Вам нужно инициализировать веса случайным вектором из нормального распределения. Ниже приведен шаблон, который должен содержать код, реализующий все варианты моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oI39UzCLy-Qu"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, reg_coef=0.4, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "        \n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.reg_coef = reg_coef\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        def stop_condition(current_iter, last_w):\n",
    "            norm = np.linalg.norm(self.w - last_w)\n",
    "            return current_iter < self.max_iter and norm > self.tolerance or current_iter == 0\n",
    "                         \n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.random.normal(size=X.shape[1])\n",
    "\n",
    "        self.w = last_w = self.w0.copy()\n",
    "        self.loss_history = []\n",
    "        current_iter = 0 \n",
    "        \n",
    "        if self.gd_type == \"GradientDescent\":\n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()\n",
    "                \n",
    "                gradient = self.calc_gradient(X, y)\n",
    "                \n",
    "                self.w -= self.eta * gradient\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "            \n",
    "        elif self.gd_type == \"StochasticDescent\":\n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()\n",
    "                \n",
    "                batch = np.random.choice(y.shape[0], int(self.delta * y.shape[0]))\n",
    "                \n",
    "                gradient = self.calc_gradient(X.iloc[batch], y.iloc[batch])\n",
    "                \n",
    "                self.w -= self.eta * gradient\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                 \n",
    "        elif self.gd_type == 'Momentum':\n",
    "            h = 0 \n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()\n",
    "                \n",
    "                gradient = self.calc_gradient(X, y)\n",
    "                h = h * self.alpha + self.eta * gradient\n",
    "                \n",
    "                self.w -= h\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                \n",
    "        elif self.gd_type == \"Adagrad\":\n",
    "            G = 0\n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()            \n",
    "                \n",
    "                gradient = self.calc_gradient(X, y)\n",
    "                G += gradient**2\n",
    "                self.w -= (self.eta * gradient) / np.sqrt(G + self.epsilon)\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * (np.dot(-X.T, (y.values.flatten() - X.dot(self.w))) + self.w.dot(self.reg_coef)) / y.shape[0]           \n",
    "                    \n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        return np.dot(X.dot(self.w) - y.values.flatten().T, (X.dot(self.w) - y.values)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QQJEjGVy-Qu"
   },
   "source": [
    "#### 8. [1 балл] Натренируйте и провалидируйте \"ручные\" модели на тех же даннных, сравните качество с моделями из Sklearn и StatsModels. Исследуйте влияние параметров `max_iter` и `alpha` на процесс оптимизации. Соответствует ли оно вашим ожиданиям?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns))\n",
    "X_test_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X_test), columns=X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1125694525965539\n",
      "0.7991943152759603\n"
     ]
    }
   ],
   "source": [
    "basic_grad = LinReg(gd_type=\"GradientDescent\", delta=0.5).fit(X_train_scaled, y_train)\n",
    "y_pred = basic_grad.predict(X_test_scaled)\n",
    "\n",
    "print(mean_squared_error(y_pred, y_test, squared=False))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11264628983361988\n",
      "0.798920091327975\n"
     ]
    }
   ],
   "source": [
    "stochastic_grad = LinReg(gd_type=\"StochasticDescent\", delta=0.5).fit(X_train_scaled, y_train)\n",
    "y_pred = stochastic_grad.predict(X_test_scaled)\n",
    "\n",
    "print(mean_squared_error(y_pred, y_test, squared=False))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 0.1000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-4, 3, num=8)\n",
    "searcher = GridSearchCV(LinReg(gd_type=\"Momentum\", delta=0.5), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_train_scaled, y_train)\n",
    "best_alpha = searcher.best_params_[\"alpha\"]\n",
    "print(\"Best alpha = %.4f\" % best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11253928247372062\n",
      "0.7993019380844024\n"
     ]
    }
   ],
   "source": [
    "momentum_grad = LinReg(gd_type=\"Momentum\", delta=0.5, alpha=best_alpha).fit(X_train_scaled, y_train)\n",
    "y_pred = momentum_grad.predict(X_test_scaled)\n",
    "\n",
    "print(mean_squared_error(y_pred, y_test, squared=False))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.357465966424735\n",
      "-87.0695461817522\n"
     ]
    }
   ],
   "source": [
    "adagrad_grad = LinReg(gd_type=\"Adagrad\", delta=0.5).fit(X_train_scaled, y_train)\n",
    "y_pred = adagrad_grad.predict(X_test_scaled)\n",
    "\n",
    "print(mean_squared_error(y_pred, y_test, squared=False))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "rIJNcxt_y-Qu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.357465966424735\n",
      "-87.0695461817522\n"
     ]
    }
   ],
   "source": [
    "adagrad_grad_25k_iters_omg = LinReg(gd_type=\"Adagrad\", delta=0.5, max_iter=25000, tolerance=1e-6).fit(X_train_scaled, y_train)\n",
    "y_pred = adagrad_grad.predict(X_test_scaled)\n",
    "\n",
    "print(mean_squared_error(y_pred, y_test, squared=False))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Комментарии</span>\n",
    "```\n",
    "Ну в целом, ничего особо удивительного, написанная модель работает так же как и аналоги из sklearn и statsmodels, если судить по функции потерь и метрике r^2. Кроме того, я решил протестировать значение max_iter на самой долгой в плане обучения модели - Adargar, правда мы и смогли получить лишь 0.01 улучшения метрики r^2, что непозволительно мало, если брать в расчет количество операций, потраченных на такое незначительное улучшение, кроме того, данное улучшение модели оооочень медленно приближалось к гиперпараметру tolerance. Вот это было для меня неожиданностью. Мне не хватило 25000 операций, чтобы приблизится к значению 10^-6.\n",
    "По поводу значения aplha, я предполагал, что число должно находиться где-то в районе 0.1..1, потому что 1 - слишком много, а при приближении к 0 очень долго бы происходило обучение, но по факту вышло предполагаемое мной значение\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqYtVqv-y-Qu"
   },
   "source": [
    "#### 9. [1 балл] Постройте графики (там же) зависимости значения функции потерь от номера итерации для всех моделей (полного градиентого спуска, стохастического гс, Momentum и Adagrad). Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "\n",
    "Не забывайте о том, как должен выглядеть *красивый* график!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Xbwhu8BSy-Qu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.collections import EventCollection\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# create random data\n",
    "xdata = np.random.random([2, 10])\n",
    "\n",
    "# split the data into two parts\n",
    "xdata1 = xdata[0, :]\n",
    "xdata2 = xdata[1, :]\n",
    "\n",
    "# sort the data so it makes clean curves\n",
    "xdata1.sort()\n",
    "xdata2.sort()\n",
    "\n",
    "# create some y data points\n",
    "ydata1 = xdata1 ** 2\n",
    "ydata2 = 1 - xdata2 ** 3\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(range((len(basic_grad.loss_history))), basic_grad.loss_history, label = \"Basic GD\")\n",
    "ax.plot(range(len(stochastic_grad.loss_history)), stochastic_grad.loss_history, label = \"Stochastic\")\n",
    "ax.plot(range(len(momentum_grad.loss_history)), momentum_grad.loss_history, label = \"Momentum_grad\")\n",
    "ax.plot(range(len(adagrad_grad.loss_history)), adagrad_grad.loss_history, label = \"Adagrad\")\n",
    "ax.plot(range(len(adagrad_grad_25k_values_omg.loss_history)), adagrad_grad_25k_values_omg.loss_history, \n",
    "        label = \"Adagrad with 25k max_iter and 10^-6 tolerance\")\n",
    "\n",
    "ax.set_title('Results')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Выводы</span>\n",
    "```\n",
    "Судя по графику, наибыстрейшим схождением обладает стохастический спуск, а худшим из представленных вышел Agarad. Все модели сходятся в целом одинаково, если не брать во внимание последний упомянутый. Думаю, это из-за очень маленьких шагов, которые выполняет эта модификация\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
