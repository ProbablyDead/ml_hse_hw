{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBrDXMdDy-Qn"
   },
   "source": [
    "# HSE 2023: Введение в машинное обучение БИ 23/24\n",
    "\n",
    "## ДЗ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXXi5K1mf41d"
   },
   "source": [
    "# Внимание!\n",
    "Если в задании просят объяснить что-либо, то это значит, что требуется письменный ответ, который является частью задания и оценивается\n",
    "\n",
    "Мы только принимаем ipynb ноутбуки. Если вы используете Google Colab, то вам необходимо скачать ноутбук перед сдачей ДЗ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "id": "mSR-a9vVy-Qp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUjuv9Qty-Qq"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Для этого ДЗ мы будем использовать датасет треков со стримингового сервиса Spotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание данных**\n",
    "\n",
    "- **track_id:** The Spotify ID for the track\n",
    "- **artists:** The artists' names who performed the track. If there is more than one artist, they are separated by a ;\n",
    "- **album_name:** The album name in which the track appears\n",
    "- **track_name:** Name of the track\n",
    "- **popularity:** The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
    "- **duration_ms:** The track length in milliseconds\n",
    "- **explicit:** Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)\n",
    "- **danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\n",
    "- **key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1\n",
    "- **loudness:** The overall loudness of a track in decibels (dB)\n",
    "- **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0\n",
    "- **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks\n",
    "- **acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\n",
    "- **instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content\n",
    "- **liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live\n",
    "- **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\n",
    "- **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
    "- **time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
    "- **track_genre:** The genre in which the track belongs\n",
    "\n",
    "**Целевая переменная**\n",
    "- **energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tHWSWTXDy-Qq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Count of rows, contains Nan values: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Посмотрим датасет на наличие в нем Nan\n",
    "print(f\" * Count of rows, contains Nan values: {data[data.isna().any(axis=1)].shape[0]}\\n\")\n",
    "\n",
    "# Всего одна строка, просто удалим ее\n",
    "data = data.dropna()\n",
    "\n",
    "y = data['energy']\n",
    "X = data.drop(['energy'], axis=1)\n",
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K81w8s35y-Qq"
   },
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYgEN-FMy-Qr"
   },
   "source": [
    "#### 0. [0.25 балла] Закодируйте категориальные признаки. Объясните выбранный вами метод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "```\n",
    "Имена авторов и жанры я дропать не стал, так как они содержат полезную информацию. \n",
    "Большинство артистов пишут музыку в определенном узнаваемом стиле, потому, увидев в строке Rammstein или Depeche mode, \n",
    "можно предсказать энергетику песни. Ну с жанрами тоже самое. Я решил закодировать эти признаки с помощью target encoding, потому что \n",
    "они не является упорядоченными и содержат большое количество значений, что не очень хочется превращать в дополнительные колонки\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-IrSlQaWy-Qr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Categorical: ['artists', 'album_name', 'track_name', 'track_genre']\n",
      " * Unique values: \n",
      "\tArtists - 31437 \n",
      "\tGenres - 114\n"
     ]
    }
   ],
   "source": [
    "# Найдем все категориальные признаки\n",
    "categorical = list(X.dtypes[X.dtypes == \"object\"].index)\n",
    "print(f\" * Categorical: {categorical}\")\n",
    "\n",
    "# Я дропнул признаки с названиями, потому что они не несут никакой прикладной информации об энергичности трека.\n",
    "X = X.drop([\"album_name\", \"track_name\"], axis=1)\n",
    "categorical.remove(\"album_name\")\n",
    "categorical.remove(\"track_name\")\n",
    "\n",
    "print(f\" * Unique values: \\n\\tArtists - {X['artists'].nunique()} \\n\\tGenres - {X['track_genre'].nunique()}\")\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "target_encoder = TargetEncoder()\n",
    "X_encoded = target_encoder.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dVwP45Gy-Qr"
   },
   "source": [
    "#### 1. [0.25 балла] Разбейте данные на train и test с пропорцией 75:25 и random_state=7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U7z8TIh5y-Qs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7daIQRfKy-Qs",
    "tags": []
   },
   "source": [
    "#### 2. [0.75 балла] Обучите модели на train'е, исключив категориальные признаки, используя библиотеку StatsModels и примените ее к test'у; используйте $RMSE$ и $R ^ 2$ в качестве метрики качества. Попробуйте также применить реализации линейной регрессии из sklearn:\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.03$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.05$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.4$\n",
    "\n",
    "Не забывайте скейлить данные с помощью StandardScaler перед обучением моделей! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, обучающая модели\n",
    "def train_on_models (X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Масштабируем признаки\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(data=scaler.transform(X_test), columns=X_test.columns)\n",
    "    y_train = list(y_train)\n",
    "    y_test = list(y_test)\n",
    "\n",
    "    X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "    X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "    \n",
    "    models = [ \n",
    "               (\"LinearRegressionModel\", sm.OLS(y_train, X_train_scaled).fit(), # statsmodels\n",
    "                LinearRegression().fit(X_train_scaled, y_train)), # sklearn\n",
    "               (\"RidgeModel\\t\", sm.OLS(y_train, X_train_scaled).fit_regularized(method='elastic_net', # statsmodels\n",
    "                                                                                    L1_wt=0.0000001, alpha=0.03, refit=True),\n",
    "                Ridge(alpha=0.03).fit(X_train_scaled, y_train)), # sklearn\n",
    "               (\"LassoModel\\t\", sm.OLS(y_train, X_train_scaled).fit_regularized(method='elastic_net', # statsmodels\n",
    "                                                                                    L1_wt=1, alpha=0.05, refit=True),\n",
    "                Lasso(alpha=0.05).fit(X_train_scaled, y_train)), # sklearn\n",
    "               (\"ElasricNetModel\\t\", sm.OLS(y_train, X_train_scaled).fit_regularized(method='elastic_net', # statsmodels\n",
    "                                                                                    L1_wt=0.4, alpha=0.01, refit=True),\n",
    "                ElasticNet(alpha=0.01, l1_ratio=0.4).fit(X_train_scaled, y_train)) # sklearn\n",
    "             ]\n",
    "    print(3*'\\t' + \"StatsModels:\" + '\\t' + \"Sklearn:\")\n",
    "    \n",
    "    for model in models: \n",
    "        y_pred_statsmodels = model[1].predict(X_test_scaled)\n",
    "        y_pred_sklean = model[2].predict(X_test_scaled)\n",
    "        \n",
    "        print(f\"{model[0]}\", end='')\n",
    "        print(\"\\tRMSE = %.4f\" % mean_squared_error(y_test, y_pred_statsmodels, squared=False), end='')\n",
    "        print(\"\\tRMSE = %.4f\" % mean_squared_error(y_test, y_pred_sklean, squared=False))\n",
    "\n",
    "    for model in models:\n",
    "        y_pred_statsmodels = model[1].predict(X_test_scaled)\n",
    "        y_pred_sklean = model[2].predict(X_test_scaled)\n",
    "        \n",
    "        print(f\"{model[0]}\", end='')\n",
    "        print(\"\\tR^2 = %.4f\" % r2_score(y_test, y_pred_statsmodels), end='')\n",
    "        print(\"\\tR^2 = %.4f\" % r2_score(y_test, y_pred_sklean))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Bkbr5iFCy-Qs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tStatsModels:\tSklearn:\n",
      "LinearRegressionModel\tRMSE = 0.1215\tRMSE = 0.1215\n",
      "RidgeModel\t\tRMSE = 0.1215\tRMSE = 0.1215\n",
      "LassoModel\t\tRMSE = 0.1367\tRMSE = 0.1473\n",
      "ElasricNetModel\t\tRMSE = 0.1218\tRMSE = 0.1225\n",
      "LinearRegressionModel\tR^2 = 0.7659\tR^2 = 0.7659\n",
      "RidgeModel\t\tR^2 = 0.7659\tR^2 = 0.7659\n",
      "LassoModel\t\tR^2 = 0.7041\tR^2 = 0.6561\n",
      "ElasricNetModel\t\tR^2 = 0.7648\tR^2 = 0.7621\n"
     ]
    }
   ],
   "source": [
    "# Получим все некатегориальные признаки \n",
    "X_train_wo_cat = X_train.drop(categorical, axis=1)\n",
    "X_test_wo_cat = X_test.drop(categorical, axis=1)\n",
    "\n",
    "models_wo_cat = train_on_models(X_train_wo_cat, X_test_wo_cat, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [0.25 балла] Повторите шаги из предыдущего пункта, добавив категориальные признаки. Прокомментируйте изменения значений метрик качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "``` \n",
    "Метрика RMSE стала меньше, а R^2 больше, что свидетельствует о повышении качества обученной модели.\n",
    "В целом ожидаемо, ведь, как я раньше писал, данные категориальные признаки содержат полезную информацию\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tStatsModels:\tSklearn:\n",
      "LinearRegressionModel\tRMSE = 0.1123\tRMSE = 0.1123\n",
      "RidgeModel\t\tRMSE = 0.1123\tRMSE = 0.1123\n",
      "LassoModel\t\tRMSE = 0.1258\tRMSE = 0.1374\n",
      "ElasricNetModel\t\tRMSE = 0.1156\tRMSE = 0.1133\n",
      "LinearRegressionModel\tR^2 = 0.8002\tR^2 = 0.8002\n",
      "RidgeModel\t\tR^2 = 0.8002\tR^2 = 0.8002\n",
      "LassoModel\t\tR^2 = 0.7493\tR^2 = 0.7009\n",
      "ElasricNetModel\t\tR^2 = 0.7881\tR^2 = 0.7966\n"
     ]
    }
   ],
   "source": [
    "models = train_on_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69JOftKRy-Qt"
   },
   "source": [
    "#### 4. [1 балл] Исследуйте значения параметров полученных моделей и проверьте какие веса получились нулевыми. Прокомментируйте значимость коэффициентов, обшую значимость модели и остальные факторы из результирующей таблицы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np1biYQ7y-Qt"
   },
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "```\n",
    "Как мы можем наблюдать, абсолютно все параметры, кроме Key имеют pvalue равное 0, что говорит нам о \n",
    "статистической важности этих параметров для модели, кроме того, вышло так, что я выбрал удачные категориальные признаки\n",
    "и выбрал удачный вариант их кодирования\n",
    "\n",
    "Значение коэффициентов при параметрах указывают на направление влияния на модель. То есть, отрицательные значения \n",
    "параметров popularity, explicit, danceability, mode, acousticness говорят о отрицательном влиянии на результирующую\n",
    "модель. Но вот что странно, я попробовал обучить модель, выбросив эти параметры, \n",
    "и метрика R-squared приняла значение 0.756, что меньше текущего, что говорит о ухудшемся качестве модели (на изображении в след ячейке)\n",
    "Как я понял, дело в том, что Key стало больше влиять на модель, но ее коффициент остался очень маленьким\n",
    "\n",
    "Можем заметить, что подобная тенденция сохраняется для всех моделей\n",
    "\n",
    "Как я понял, nan значения p появляются когда вероятность равна 0 или 1, в нашем случае p = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.802</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-131902.0414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 23:08</td>        <td>BIC:</td>         <td>-131742.9850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>65968.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>16</td>           <td>F-statistic:</td>       <td>2.169e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85482</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.802</td>            <td>Scale:</td>          <td>0.012515</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1677.8057</td> <td>0.0000</td> <td>0.6412</td>  <td>0.6427</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0508</td>   <td>0.0005</td>   <td>94.6209</td>  <td>0.0000</td> <td>0.0498</td>  <td>0.0519</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>-0.0029</td>  <td>0.0004</td>   <td>-7.4317</td>  <td>0.0000</td> <td>-0.0036</td> <td>-0.0021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0017</td>   <td>0.0004</td>   <td>4.2190</td>   <td>0.0000</td> <td>0.0009</td>  <td>0.0024</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>-0.0027</td>  <td>0.0004</td>   <td>-6.4926</td>  <td>0.0000</td> <td>-0.0035</td> <td>-0.0019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>-0.0269</td>  <td>0.0005</td>  <td>-57.4729</td>  <td>0.0000</td> <td>-0.0279</td> <td>-0.0260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0004</td>   <td>0.0004</td>   <td>1.1541</td>   <td>0.2485</td> <td>-0.0003</td> <td>0.0012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1113</td>   <td>0.0006</td>  <td>191.7855</td>  <td>0.0000</td> <td>0.1102</td>  <td>0.1125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>-0.0026</td>  <td>0.0004</td>   <td>-6.6334</td>  <td>0.0000</td> <td>-0.0034</td> <td>-0.0018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0217</td>   <td>0.0004</td>   <td>51.8706</td>  <td>0.0000</td> <td>0.0209</td>  <td>0.0225</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0744</td>  <td>0.0006</td>  <td>-132.5116</td> <td>0.0000</td> <td>-0.0755</td> <td>-0.0733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0334</td>   <td>0.0005</td>   <td>73.8899</td>  <td>0.0000</td> <td>0.0325</td>  <td>0.0343</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0212</td>   <td>0.0004</td>   <td>52.5667</td>  <td>0.0000</td> <td>0.0204</td>  <td>0.0219</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0376</td>   <td>0.0005</td>   <td>80.2445</td>  <td>0.0000</td> <td>0.0366</td>  <td>0.0385</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0056</td>   <td>0.0004</td>   <td>13.8626</td>  <td>0.0000</td> <td>0.0048</td>  <td>0.0063</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0050</td>   <td>0.0004</td>   <td>12.5746</td>  <td>0.0000</td> <td>0.0042</td>  <td>0.0058</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0260</td>   <td>0.0006</td>   <td>46.0216</td>  <td>0.0000</td> <td>0.0249</td>  <td>0.0271</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>5424.939</td>  <td>Durbin-Watson:</td>     <td>1.998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>23851.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.133</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>5.574</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.802         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -131902.0414  \\\\\n",
       "Date:               & 2023-10-23 23:08 & BIC:                & -131742.9850  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 65968.        \\\\\n",
       "Df Model:           & 16               & F-statistic:        & 2.169e+04     \\\\\n",
       "Df Residuals:       & 85482            & Prob (F-statistic): & 0.00          \\\\\n",
       "R-squared:          & 0.802            & Scale:              & 0.012515      \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1677.8057 &      0.0000 &  0.6412 &  0.6427  \\\\\n",
       "artists          &  0.0508 &   0.0005 &   94.6209 &      0.0000 &  0.0498 &  0.0519  \\\\\n",
       "popularity       & -0.0029 &   0.0004 &   -7.4317 &      0.0000 & -0.0036 & -0.0021  \\\\\n",
       "duration\\_ms     &  0.0017 &   0.0004 &    4.2190 &      0.0000 &  0.0009 &  0.0024  \\\\\n",
       "explicit         & -0.0027 &   0.0004 &   -6.4926 &      0.0000 & -0.0035 & -0.0019  \\\\\n",
       "danceability     & -0.0269 &   0.0005 &  -57.4729 &      0.0000 & -0.0279 & -0.0260  \\\\\n",
       "key              &  0.0004 &   0.0004 &    1.1541 &      0.2485 & -0.0003 &  0.0012  \\\\\n",
       "loudness         &  0.1113 &   0.0006 &  191.7855 &      0.0000 &  0.1102 &  0.1125  \\\\\n",
       "mode             & -0.0026 &   0.0004 &   -6.6334 &      0.0000 & -0.0034 & -0.0018  \\\\\n",
       "speechiness      &  0.0217 &   0.0004 &   51.8706 &      0.0000 &  0.0209 &  0.0225  \\\\\n",
       "acousticness     & -0.0744 &   0.0006 & -132.5116 &      0.0000 & -0.0755 & -0.0733  \\\\\n",
       "instrumentalness &  0.0334 &   0.0005 &   73.8899 &      0.0000 &  0.0325 &  0.0343  \\\\\n",
       "liveness         &  0.0212 &   0.0004 &   52.5667 &      0.0000 &  0.0204 &  0.0219  \\\\\n",
       "valence          &  0.0376 &   0.0005 &   80.2445 &      0.0000 &  0.0366 &  0.0385  \\\\\n",
       "tempo            &  0.0056 &   0.0004 &   13.8626 &      0.0000 &  0.0048 &  0.0063  \\\\\n",
       "time\\_signature  &  0.0050 &   0.0004 &   12.5746 &      0.0000 &  0.0042 &  0.0058  \\\\\n",
       "track\\_genre     &  0.0260 &   0.0006 &   46.0216 &      0.0000 &  0.0249 &  0.0271  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 5424.939 & Durbin-Watson:    & 1.998      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 23851.079  \\\\\n",
       "Skew:          & 0.133    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 5.574    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.802       \n",
       "Dependent Variable: y                AIC:                -131902.0414\n",
       "Date:               2023-10-23 23:08 BIC:                -131742.9850\n",
       "No. Observations:   85499            Log-Likelihood:     65968.      \n",
       "Df Model:           16               F-statistic:        2.169e+04   \n",
       "Df Residuals:       85482            Prob (F-statistic): 0.00        \n",
       "R-squared:          0.802            Scale:              0.012515    \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1677.8057 0.0000  0.6412  0.6427\n",
       "artists              0.0508   0.0005   94.6209 0.0000  0.0498  0.0519\n",
       "popularity          -0.0029   0.0004   -7.4317 0.0000 -0.0036 -0.0021\n",
       "duration_ms          0.0017   0.0004    4.2190 0.0000  0.0009  0.0024\n",
       "explicit            -0.0027   0.0004   -6.4926 0.0000 -0.0035 -0.0019\n",
       "danceability        -0.0269   0.0005  -57.4729 0.0000 -0.0279 -0.0260\n",
       "key                  0.0004   0.0004    1.1541 0.2485 -0.0003  0.0012\n",
       "loudness             0.1113   0.0006  191.7855 0.0000  0.1102  0.1125\n",
       "mode                -0.0026   0.0004   -6.6334 0.0000 -0.0034 -0.0018\n",
       "speechiness          0.0217   0.0004   51.8706 0.0000  0.0209  0.0225\n",
       "acousticness        -0.0744   0.0006 -132.5116 0.0000 -0.0755 -0.0733\n",
       "instrumentalness     0.0334   0.0005   73.8899 0.0000  0.0325  0.0343\n",
       "liveness             0.0212   0.0004   52.5667 0.0000  0.0204  0.0219\n",
       "valence              0.0376   0.0005   80.2445 0.0000  0.0366  0.0385\n",
       "tempo                0.0056   0.0004   13.8626 0.0000  0.0048  0.0063\n",
       "time_signature       0.0050   0.0004   12.5746 0.0000  0.0042  0.0058\n",
       "track_genre          0.0260   0.0006   46.0216 0.0000  0.0249  0.0271\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             5424.939       Durbin-Watson:          1.998    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       23851.079\n",
       "Skew:                0.133          Prob(JB):               0.000    \n",
       "Kurtosis:            5.574          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear regression \n",
    "models[0][1].summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"results_wo_params.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.802</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-131900.0414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 23:08</td>        <td>BIC:</td>         <td>-131731.6287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>65968.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>17</td>           <td>F-statistic:</td>       <td>2.042e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85482</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>          <td>elastic_net</td>         <td>Scale:</td>          <td>0.012515</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.802</td>               <td></td>                 <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1677.8057</td> <td>0.0000</td> <td>0.6412</td>  <td>0.6427</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0508</td>   <td>0.0005</td>   <td>94.6209</td>  <td>0.0000</td> <td>0.0498</td>  <td>0.0519</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>-0.0029</td>  <td>0.0004</td>   <td>-7.4317</td>  <td>0.0000</td> <td>-0.0036</td> <td>-0.0021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0017</td>   <td>0.0004</td>   <td>4.2190</td>   <td>0.0000</td> <td>0.0009</td>  <td>0.0024</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>-0.0027</td>  <td>0.0004</td>   <td>-6.4926</td>  <td>0.0000</td> <td>-0.0035</td> <td>-0.0019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>-0.0269</td>  <td>0.0005</td>  <td>-57.4729</td>  <td>0.0000</td> <td>-0.0279</td> <td>-0.0260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0004</td>   <td>0.0004</td>   <td>1.1541</td>   <td>0.2485</td> <td>-0.0003</td> <td>0.0012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1113</td>   <td>0.0006</td>  <td>191.7855</td>  <td>0.0000</td> <td>0.1102</td>  <td>0.1125</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>-0.0026</td>  <td>0.0004</td>   <td>-6.6334</td>  <td>0.0000</td> <td>-0.0034</td> <td>-0.0018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0217</td>   <td>0.0004</td>   <td>51.8706</td>  <td>0.0000</td> <td>0.0209</td>  <td>0.0225</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0744</td>  <td>0.0006</td>  <td>-132.5116</td> <td>0.0000</td> <td>-0.0755</td> <td>-0.0733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0334</td>   <td>0.0005</td>   <td>73.8899</td>  <td>0.0000</td> <td>0.0325</td>  <td>0.0343</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0212</td>   <td>0.0004</td>   <td>52.5667</td>  <td>0.0000</td> <td>0.0204</td>  <td>0.0219</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0376</td>   <td>0.0005</td>   <td>80.2445</td>  <td>0.0000</td> <td>0.0366</td>  <td>0.0385</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0056</td>   <td>0.0004</td>   <td>13.8626</td>  <td>0.0000</td> <td>0.0048</td>  <td>0.0063</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0050</td>   <td>0.0004</td>   <td>12.5746</td>  <td>0.0000</td> <td>0.0042</td>  <td>0.0058</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0260</td>   <td>0.0006</td>   <td>46.0216</td>  <td>0.0000</td> <td>0.0249</td>  <td>0.0271</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>5424.939</td>  <td>Durbin-Watson:</td>     <td>1.998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>23851.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.133</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>5.574</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.802         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -131900.0414  \\\\\n",
       "Date:               & 2023-10-23 23:08 & BIC:                & -131731.6287  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 65968.        \\\\\n",
       "Df Model:           & 17               & F-statistic:        & 2.042e+04     \\\\\n",
       "Df Residuals:       & 85482            & Prob (F-statistic): & 0.00          \\\\\n",
       "Method:             & elastic\\_net     & Scale:              & 0.012515      \\\\\n",
       "R-squared:          & 0.802            &                     &               \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1677.8057 &      0.0000 &  0.6412 &  0.6427  \\\\\n",
       "artists          &  0.0508 &   0.0005 &   94.6209 &      0.0000 &  0.0498 &  0.0519  \\\\\n",
       "popularity       & -0.0029 &   0.0004 &   -7.4317 &      0.0000 & -0.0036 & -0.0021  \\\\\n",
       "duration\\_ms     &  0.0017 &   0.0004 &    4.2190 &      0.0000 &  0.0009 &  0.0024  \\\\\n",
       "explicit         & -0.0027 &   0.0004 &   -6.4926 &      0.0000 & -0.0035 & -0.0019  \\\\\n",
       "danceability     & -0.0269 &   0.0005 &  -57.4729 &      0.0000 & -0.0279 & -0.0260  \\\\\n",
       "key              &  0.0004 &   0.0004 &    1.1541 &      0.2485 & -0.0003 &  0.0012  \\\\\n",
       "loudness         &  0.1113 &   0.0006 &  191.7855 &      0.0000 &  0.1102 &  0.1125  \\\\\n",
       "mode             & -0.0026 &   0.0004 &   -6.6334 &      0.0000 & -0.0034 & -0.0018  \\\\\n",
       "speechiness      &  0.0217 &   0.0004 &   51.8706 &      0.0000 &  0.0209 &  0.0225  \\\\\n",
       "acousticness     & -0.0744 &   0.0006 & -132.5116 &      0.0000 & -0.0755 & -0.0733  \\\\\n",
       "instrumentalness &  0.0334 &   0.0005 &   73.8899 &      0.0000 &  0.0325 &  0.0343  \\\\\n",
       "liveness         &  0.0212 &   0.0004 &   52.5667 &      0.0000 &  0.0204 &  0.0219  \\\\\n",
       "valence          &  0.0376 &   0.0005 &   80.2445 &      0.0000 &  0.0366 &  0.0385  \\\\\n",
       "tempo            &  0.0056 &   0.0004 &   13.8626 &      0.0000 &  0.0048 &  0.0063  \\\\\n",
       "time\\_signature  &  0.0050 &   0.0004 &   12.5746 &      0.0000 &  0.0042 &  0.0058  \\\\\n",
       "track\\_genre     &  0.0260 &   0.0006 &   46.0216 &      0.0000 &  0.0249 &  0.0271  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 5424.939 & Durbin-Watson:    & 1.998      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 23851.079  \\\\\n",
       "Skew:          & 0.133    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 5.574    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.802       \n",
       "Dependent Variable: y                AIC:                -131900.0414\n",
       "Date:               2023-10-23 23:08 BIC:                -131731.6287\n",
       "No. Observations:   85499            Log-Likelihood:     65968.      \n",
       "Df Model:           17               F-statistic:        2.042e+04   \n",
       "Df Residuals:       85482            Prob (F-statistic): 0.00        \n",
       "Method:             elastic_net      Scale:              0.012515    \n",
       "R-squared:          0.802                                            \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1677.8057 0.0000  0.6412  0.6427\n",
       "artists              0.0508   0.0005   94.6209 0.0000  0.0498  0.0519\n",
       "popularity          -0.0029   0.0004   -7.4317 0.0000 -0.0036 -0.0021\n",
       "duration_ms          0.0017   0.0004    4.2190 0.0000  0.0009  0.0024\n",
       "explicit            -0.0027   0.0004   -6.4926 0.0000 -0.0035 -0.0019\n",
       "danceability        -0.0269   0.0005  -57.4729 0.0000 -0.0279 -0.0260\n",
       "key                  0.0004   0.0004    1.1541 0.2485 -0.0003  0.0012\n",
       "loudness             0.1113   0.0006  191.7855 0.0000  0.1102  0.1125\n",
       "mode                -0.0026   0.0004   -6.6334 0.0000 -0.0034 -0.0018\n",
       "speechiness          0.0217   0.0004   51.8706 0.0000  0.0209  0.0225\n",
       "acousticness        -0.0744   0.0006 -132.5116 0.0000 -0.0755 -0.0733\n",
       "instrumentalness     0.0334   0.0005   73.8899 0.0000  0.0325  0.0343\n",
       "liveness             0.0212   0.0004   52.5667 0.0000  0.0204  0.0219\n",
       "valence              0.0376   0.0005   80.2445 0.0000  0.0366  0.0385\n",
       "tempo                0.0056   0.0004   13.8626 0.0000  0.0048  0.0063\n",
       "time_signature       0.0050   0.0004   12.5746 0.0000  0.0042  0.0058\n",
       "track_genre          0.0260   0.0006   46.0216 0.0000  0.0249  0.0271\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             5424.939       Durbin-Watson:          1.998    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       23851.079\n",
       "Skew:                0.133          Prob(JB):               0.000    \n",
       "Kurtosis:            5.574          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge\n",
    "models[1][1].summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.752</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-112500.0136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 23:08</td>        <td>BIC:</td>         <td>-112453.2323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>56255.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>4</td>           <td>F-statistic:</td>       <td>6.480e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85495</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>          <td>elastic_net</td>         <td>Scale:</td>          <td>0.015705</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.752</td>               <td></td>                 <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1497.7425</td> <td>0.0000</td> <td>0.6411</td>  <td>0.6428</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0709</td>   <td>0.0006</td>  <td>127.8784</td>  <td>0.0000</td> <td>0.0698</td>  <td>0.0720</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1044</td>   <td>0.0006</td>  <td>186.7080</td>  <td>0.0000</td> <td>0.1033</td>  <td>0.1055</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0821</td>  <td>0.0006</td>  <td>-143.5493</td> <td>0.0000</td> <td>-0.0833</td> <td>-0.0810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>4212.831</td>  <td>Durbin-Watson:</td>     <td>1.997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>11879.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.239</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>4.762</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.752         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -112500.0136  \\\\\n",
       "Date:               & 2023-10-23 23:08 & BIC:                & -112453.2323  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 56255.        \\\\\n",
       "Df Model:           & 4                & F-statistic:        & 6.480e+04     \\\\\n",
       "Df Residuals:       & 85495            & Prob (F-statistic): & 0.00          \\\\\n",
       "Method:             & elastic\\_net     & Scale:              & 0.015705      \\\\\n",
       "R-squared:          & 0.752            &                     &               \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1497.7425 &      0.0000 &  0.6411 &  0.6428  \\\\\n",
       "artists          &  0.0709 &   0.0006 &  127.8784 &      0.0000 &  0.0698 &  0.0720  \\\\\n",
       "popularity       &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "duration\\_ms     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "explicit         &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "danceability     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "key              &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "loudness         &  0.1044 &   0.0006 &  186.7080 &      0.0000 &  0.1033 &  0.1055  \\\\\n",
       "mode             &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "speechiness      &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "acousticness     & -0.0821 &   0.0006 & -143.5493 &      0.0000 & -0.0833 & -0.0810  \\\\\n",
       "instrumentalness &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "liveness         &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "valence          &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "tempo            &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "time\\_signature  &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "track\\_genre     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 4212.831 & Durbin-Watson:    & 1.997      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 11879.118  \\\\\n",
       "Skew:          & 0.239    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 4.762    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.752       \n",
       "Dependent Variable: y                AIC:                -112500.0136\n",
       "Date:               2023-10-23 23:08 BIC:                -112453.2323\n",
       "No. Observations:   85499            Log-Likelihood:     56255.      \n",
       "Df Model:           4                F-statistic:        6.480e+04   \n",
       "Df Residuals:       85495            Prob (F-statistic): 0.00        \n",
       "Method:             elastic_net      Scale:              0.015705    \n",
       "R-squared:          0.752                                            \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1497.7425 0.0000  0.6411  0.6428\n",
       "artists              0.0709   0.0006  127.8784 0.0000  0.0698  0.0720\n",
       "popularity           0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "duration_ms          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "explicit             0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "danceability         0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "key                  0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "loudness             0.1044   0.0006  186.7080 0.0000  0.1033  0.1055\n",
       "mode                 0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "speechiness          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "acousticness        -0.0821   0.0006 -143.5493 0.0000 -0.0833 -0.0810\n",
       "instrumentalness     0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "liveness             0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "valence              0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "tempo                0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "time_signature       0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "track_genre          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             4212.831       Durbin-Watson:          1.997    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       11879.118\n",
       "Skew:                0.239          Prob(JB):               0.000    \n",
       "Kurtosis:            4.762          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso\n",
    "models[2][1].summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>       <td>0.790</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>-126585.0528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-10-23 23:08</td>        <td>BIC:</td>         <td>-126482.1339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>85499</td>        <td>Log-Likelihood:</td>      <td>63304.</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>10</td>           <td>F-statistic:</td>       <td>3.210e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>85489</td>      <td>Prob (F-statistic):</td>     <td>0.00</td>    \n",
       "</tr>\n",
       "<tr>\n",
       "        <td>Method:</td>          <td>elastic_net</td>         <td>Scale:</td>          <td>0.013319</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.790</td>               <td></td>                 <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>      <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>0.6419</td>   <td>0.0004</td>  <td>1626.3914</td> <td>0.0000</td> <td>0.6411</td>  <td>0.6427</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>artists</th>          <td>0.0626</td>   <td>0.0005</td>  <td>121.0446</td>  <td>0.0000</td> <td>0.0615</td>  <td>0.0636</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>popularity</th>       <td>-0.0044</td>  <td>0.0004</td>  <td>-10.9294</td>  <td>0.0000</td> <td>-0.0051</td> <td>-0.0036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration_ms</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>explicit</th>         <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>danceability</th>     <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>key</th>              <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>loudness</th>         <td>0.1124</td>   <td>0.0006</td>  <td>192.9112</td>  <td>0.0000</td> <td>0.1113</td>  <td>0.1135</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mode</th>             <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speechiness</th>      <td>0.0191</td>   <td>0.0004</td>   <td>46.9316</td>  <td>0.0000</td> <td>0.0183</td>  <td>0.0199</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acousticness</th>     <td>-0.0804</td>  <td>0.0005</td>  <td>-148.7908</td> <td>0.0000</td> <td>-0.0815</td> <td>-0.0794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>instrumentalness</th> <td>0.0338</td>   <td>0.0005</td>   <td>72.9604</td>  <td>0.0000</td> <td>0.0329</td>  <td>0.0347</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>liveness</th>         <td>0.0261</td>   <td>0.0004</td>   <td>64.2627</td>  <td>0.0000</td> <td>0.0253</td>  <td>0.0269</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>valence</th>          <td>0.0250</td>   <td>0.0004</td>   <td>58.8128</td>  <td>0.0000</td> <td>0.0241</td>  <td>0.0258</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tempo</th>            <td>0.0101</td>   <td>0.0004</td>   <td>24.7592</td>  <td>0.0000</td> <td>0.0093</td>  <td>0.0109</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_signature</th>   <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>track_genre</th>      <td>0.0000</td>   <td>0.0000</td>     <td>nan</td>      <td>nan</td>  <td>0.0000</td>  <td>0.0000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>4400.795</td>  <td>Durbin-Watson:</td>     <td>1.996</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>   <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>16699.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>       <td>0.085</td>      <td>Prob(JB):</td>       <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>     <td>5.158</td>   <td>Condition No.:</td>       <td>3</td>    \n",
       "</tr>\n",
       "</table><br/>\n",
       "Notes:<br/>\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{table}\n",
       "\\caption{Results: Ordinary least squares}\n",
       "\\label{}\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Model:              & OLS              & Adj. R-squared:     & 0.790         \\\\\n",
       "Dependent Variable: & y                & AIC:                & -126585.0528  \\\\\n",
       "Date:               & 2023-10-23 23:08 & BIC:                & -126482.1339  \\\\\n",
       "No. Observations:   & 85499            & Log-Likelihood:     & 63304.        \\\\\n",
       "Df Model:           & 10               & F-statistic:        & 3.210e+04     \\\\\n",
       "Df Residuals:       & 85489            & Prob (F-statistic): & 0.00          \\\\\n",
       "Method:             & elastic\\_net     & Scale:              & 0.013319      \\\\\n",
       "R-squared:          & 0.790            &                     &               \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{lrrrrrr}\n",
       "\\hline\n",
       "                 &   Coef. & Std.Err. &         t & P$> |$t$|$ &  [0.025 &  0.975]  \\\\\n",
       "\\hline\n",
       "const            &  0.6419 &   0.0004 & 1626.3914 &      0.0000 &  0.6411 &  0.6427  \\\\\n",
       "artists          &  0.0626 &   0.0005 &  121.0446 &      0.0000 &  0.0615 &  0.0636  \\\\\n",
       "popularity       & -0.0044 &   0.0004 &  -10.9294 &      0.0000 & -0.0051 & -0.0036  \\\\\n",
       "duration\\_ms     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "explicit         &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "danceability     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "key              &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "loudness         &  0.1124 &   0.0006 &  192.9112 &      0.0000 &  0.1113 &  0.1135  \\\\\n",
       "mode             &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "speechiness      &  0.0191 &   0.0004 &   46.9316 &      0.0000 &  0.0183 &  0.0199  \\\\\n",
       "acousticness     & -0.0804 &   0.0005 & -148.7908 &      0.0000 & -0.0815 & -0.0794  \\\\\n",
       "instrumentalness &  0.0338 &   0.0005 &   72.9604 &      0.0000 &  0.0329 &  0.0347  \\\\\n",
       "liveness         &  0.0261 &   0.0004 &   64.2627 &      0.0000 &  0.0253 &  0.0269  \\\\\n",
       "valence          &  0.0250 &   0.0004 &   58.8128 &      0.0000 &  0.0241 &  0.0258  \\\\\n",
       "tempo            &  0.0101 &   0.0004 &   24.7592 &      0.0000 &  0.0093 &  0.0109  \\\\\n",
       "time\\_signature  &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "track\\_genre     &  0.0000 &   0.0000 &       nan &         nan &  0.0000 &  0.0000  \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\n",
       "\\begin{center}\n",
       "\\begin{tabular}{llll}\n",
       "\\hline\n",
       "Omnibus:       & 4400.795 & Durbin-Watson:    & 1.996      \\\\\n",
       "Prob(Omnibus): & 0.000    & Jarque-Bera (JB): & 16699.588  \\\\\n",
       "Skew:          & 0.085    & Prob(JB):         & 0.000      \\\\\n",
       "Kurtosis:      & 5.158    & Condition No.:    & 3          \\\\\n",
       "\\hline\n",
       "\\end{tabular}\n",
       "\\end{center}\n",
       "\\end{table}\n",
       "\\bigskip\n",
       "Notes: \\newline \n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                   Results: Ordinary least squares\n",
       "=====================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.790       \n",
       "Dependent Variable: y                AIC:                -126585.0528\n",
       "Date:               2023-10-23 23:08 BIC:                -126482.1339\n",
       "No. Observations:   85499            Log-Likelihood:     63304.      \n",
       "Df Model:           10               F-statistic:        3.210e+04   \n",
       "Df Residuals:       85489            Prob (F-statistic): 0.00        \n",
       "Method:             elastic_net      Scale:              0.013319    \n",
       "R-squared:          0.790                                            \n",
       "---------------------------------------------------------------------\n",
       "                     Coef.  Std.Err.     t     P>|t|   [0.025  0.975]\n",
       "---------------------------------------------------------------------\n",
       "const                0.6419   0.0004 1626.3914 0.0000  0.6411  0.6427\n",
       "artists              0.0626   0.0005  121.0446 0.0000  0.0615  0.0636\n",
       "popularity          -0.0044   0.0004  -10.9294 0.0000 -0.0051 -0.0036\n",
       "duration_ms          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "explicit             0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "danceability         0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "key                  0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "loudness             0.1124   0.0006  192.9112 0.0000  0.1113  0.1135\n",
       "mode                 0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "speechiness          0.0191   0.0004   46.9316 0.0000  0.0183  0.0199\n",
       "acousticness        -0.0804   0.0005 -148.7908 0.0000 -0.0815 -0.0794\n",
       "instrumentalness     0.0338   0.0005   72.9604 0.0000  0.0329  0.0347\n",
       "liveness             0.0261   0.0004   64.2627 0.0000  0.0253  0.0269\n",
       "valence              0.0250   0.0004   58.8128 0.0000  0.0241  0.0258\n",
       "tempo                0.0101   0.0004   24.7592 0.0000  0.0093  0.0109\n",
       "time_signature       0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "track_genre          0.0000   0.0000       nan    nan  0.0000  0.0000\n",
       "---------------------------------------------------------------------\n",
       "Omnibus:             4400.795       Durbin-Watson:          1.996    \n",
       "Prob(Omnibus):       0.000          Jarque-Bera (JB):       16699.588\n",
       "Skew:                0.085          Prob(JB):               0.000    \n",
       "Kurtosis:            5.158          Condition No.:          3        \n",
       "=====================================================================\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors\n",
       "is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ElasricNet\n",
    "models[3][1].summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLcvGlUZy-Qt"
   },
   "source": [
    "#### 5. [1 балл] Реализуйте один из алгоритмов отбора признаков (Elimination by P-value, Forward elimination, Backward elimination), сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Объяснения</span>\n",
    "```\n",
    "Для проверки алгоритма Backward elimination я закинул в функцию исходные данные, со всеми признаками, закодированными target encode,\n",
    "чтобы проверить качество обученной модели, если дать модели самой решить какие признаки нужны, а какие, напротив, мешают\n",
    "\n",
    "Так же, для проверки работоспособности алгоритма я закинул в него максимальное значение p = 0, хотя на практике я бы выбрал p = 0.05\n",
    "\n",
    "Пусть метрика r^2 adj осталась неизменной, зато мы избавились от переменных, которые не влияли на результат,\n",
    "а значит облегчили модель\n",
    "\n",
    "Но, по сути, если бы у нас были явно плохие параметры, имеющие очень большое p, то удаление данного параметра явно оказало бы на модель положительный эффект\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination (X, y, max_p_value):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns))\n",
    "    y_train = list(y)\n",
    "    \n",
    "    model = sm.OLS(y_train, X_train_scaled).fit()\n",
    "    print(f\"Before - r^2 adj = {model.rsquared_adj}\\n\")\n",
    "    print(model.pvalues)\n",
    "    \n",
    "    while (max(model.pvalues) > max_p_value):\n",
    "        params_w_p_greater_than_given = model.pvalues[model.pvalues == max(model.pvalues)].keys()\n",
    "        X_train_scaled = X_train_scaled.drop(params_w_p_greater_than_given, axis=1)\n",
    "\n",
    "        model = sm.OLS(y_train, X_train_scaled).fit()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tmp = data['energy']\n",
    "X_tmp = data.drop(['energy'], axis=1)\n",
    "target_encoder = TargetEncoder()\n",
    "X_encoded_tmp = target_encoder.fit_transform(X_tmp, y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TnrbRbkwy-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before - r^2 adj = 0.8534447110006806\n",
      "\n",
      "const                0.000000e+00\n",
      "artists             1.915545e-263\n",
      "album_name           0.000000e+00\n",
      "track_name           0.000000e+00\n",
      "popularity           8.719158e-04\n",
      "duration_ms          5.288067e-03\n",
      "explicit             1.030235e-13\n",
      "danceability         0.000000e+00\n",
      "key                  2.268750e-04\n",
      "loudness             0.000000e+00\n",
      "mode                 2.459871e-02\n",
      "speechiness          0.000000e+00\n",
      "acousticness         0.000000e+00\n",
      "instrumentalness     0.000000e+00\n",
      "liveness             0.000000e+00\n",
      "valence              0.000000e+00\n",
      "tempo                9.145288e-36\n",
      "time_signature       4.227223e-34\n",
      "track_genre          0.000000e+00\n",
      "dtype: float64\n",
      "\n",
      "After - r^2 adj = 0.8513892068505113\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "const               0.0\n",
       "album_name          0.0\n",
       "track_name          0.0\n",
       "danceability        0.0\n",
       "loudness            0.0\n",
       "speechiness         0.0\n",
       "acousticness        0.0\n",
       "instrumentalness    0.0\n",
       "liveness            0.0\n",
       "valence             0.0\n",
       "track_genre         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue = 0\n",
    "# Тут для проверки реализации я задал значение p равное 0, но по факту оптимальным будет 0.05\n",
    "\n",
    "result_model = backward_elimination(X_encoded_tmp, y_tmp, pvalue)\n",
    "\n",
    "print(f\"\\nAfter - r^2 adj = {result_model.rsquared_adj}\\n\")\n",
    "result_model.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df0eQLdNy-Qt"
   },
   "source": [
    "#### 6. [1 балл] Найдите лучший (по RMSE) $\\alpha$ для регрессиии Lasso, используя кросс-валидацию на 5 фолдов. Вы должны выбрать значение из промежутка $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JPoT3YHqy-Qt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 0.0001\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_encoded_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X_encoded), columns=X_encoded.columns))\n",
    "\n",
    "alphas = np.logspace(-4, 3, num=8)\n",
    "searcher = GridSearchCV(Lasso(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_encoded_scaled, y) # Данные, не разделенные на train/test\n",
    "best_alpha = searcher.best_params_[\"alpha\"]\n",
    "print(\"Best alpha = %.4f\" % best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1PKinJUy-Qt"
   },
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "#### 7. [3.5 балла] Имплементируйте  Ridge регрессию для MSE loss, обученную на градиентом спуске.\n",
    "\n",
    "\n",
    "Все вычисления должны быть векторизованы, а циклы Python можно использовать только для итераций градиентного спуска. В качестве критерия остановки необходимо использовать (одновременно):\n",
    "\n",
    "* проверка абсолютной нормы разницы весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$, заданного параметром `tolerance`);\n",
    "\n",
    "* достижение максимального количества итераций (например, 10000, заданного параметром `max_iter`).\n",
    "\n",
    "Вам необходимо выполнить:\n",
    "\n",
    "* Полный градиентный спуск:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Стохастический градиентный спуск:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ является оценкой градиента по набору объектов, выбранных случайным образом.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "Чтобы убедиться, что процесс оптимизации действительно выполняется, мы будем использовать атрибут класса `loss_history`. После вызова метода fit он должен содержать значения функции потерь для всех итераций, начиная с первой (до первого шага по антиградиенту).\n",
    "\n",
    "\n",
    "Вам нужно инициализировать веса случайным вектором из нормального распределения. Ниже приведен шаблон, который должен содержать код, реализующий все варианты моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oI39UzCLy-Qu"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, reg_coef=0.4, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "        \n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.reg_coef = reg_coef\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        def stop_condition(current_iter, last_w):\n",
    "            norm = np.linalg.norm(self.w - last_w)\n",
    "            return current_iter < self.max_iter and norm > self.tolerance or current_iter == 0\n",
    "                         \n",
    "        if self.w0 is None:\n",
    "            self.w0 = np.random.normal(size=X.shape[1])\n",
    "\n",
    "        self.w = last_w = self.w0.copy()\n",
    "        self.loss_history = []\n",
    "        current_iter = 0 \n",
    "        \n",
    "        if self.gd_type == \"GradientDescent\":\n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()\n",
    "                \n",
    "                gradient = self.calc_gradient(X, y)\n",
    "                \n",
    "                self.w -= self.eta * gradient\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "            \n",
    "        elif self.gd_type == \"StochasticDescent\":\n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()\n",
    "                \n",
    "                batch = np.random.choice(y.shape[0], int(self.delta * y.shape[0]))\n",
    "                \n",
    "                gradient = self.calc_gradient(X.iloc[batch], y.iloc[batch])\n",
    "                \n",
    "                self.w -= self.eta * gradient\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                 \n",
    "        elif self.gd_type == 'Momentum':\n",
    "            h = 0 \n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()\n",
    "                \n",
    "                gradient = self.calc_gradient(X, y)\n",
    "                h = h * self.alpha + self.eta * gradient\n",
    "                \n",
    "                self.w -= h\n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                \n",
    "        elif self.gd_type == \"Adagrad\":\n",
    "            G = 0\n",
    "            while stop_condition(current_iter, last_w):\n",
    "                current_iter += 1\n",
    "                last_w = self.w.copy()            \n",
    "                \n",
    "                gradient = self.calc_gradient(X, y)\n",
    "                G += gradient**2\n",
    "                self.w -= (gradient / np.sqrt(G + self.epsilon)) * self.eta \n",
    "                \n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * (np.dot(-X.T, (y.values.flatten() - X.dot(self.w))) + self.w.dot(self.reg_coef)) / y.shape[0]           \n",
    "                    \n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        return np.dot(X.dot(self.w) - y.values.flatten().T, (X.dot(self.w) - y.values)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QQJEjGVy-Qu"
   },
   "source": [
    "#### 8. [1 балл] Натренируйте и провалидируйте \"ручные\" модели на тех же даннных, сравните качество с моделями из Sklearn и StatsModels. Исследуйте влияние параметров `max_iter` и `alpha` на процесс оптимизации. Соответствует ли оно вашим ожиданиям?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X_train), columns=X_train.columns))\n",
    "X_test_scaled = sm.add_constant(pd.DataFrame(data=scaler.fit_transform(X_test), columns=X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 736\n",
      "MSE = 0.11268714385239838\n",
      "R^2 = 0.7987742114760701\n"
     ]
    }
   ],
   "source": [
    "basic_grad = LinReg(gd_type=\"GradientDescent\", delta=0.5).fit(X_train_scaled, y_train)\n",
    "y_pred = basic_grad.predict(X_test_scaled)\n",
    "\n",
    "print(f'Number of iterations {len(basic_grad.loss_history)}')\n",
    "print(f'MSE = {mean_squared_error(y_pred, y_test, squared=False)}')\n",
    "print(f'R^2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 636\n",
      "MSE = 0.11259622387623819\n",
      "R^2 = 0.7990987926583479\n"
     ]
    }
   ],
   "source": [
    "stochastic_grad = LinReg(gd_type=\"StochasticDescent\", delta=0.5).fit(X_train_scaled, y_train)\n",
    "y_pred = stochastic_grad.predict(X_test_scaled)\n",
    "\n",
    "print(f'Number of iterations {len(stochastic_grad.loss_history)}')\n",
    "print(f'MSE = {mean_squared_error(y_pred, y_test, squared=False)}')\n",
    "print(f'R^2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 0.1000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-4, 3, num=8)\n",
    "searcher = GridSearchCV(LinReg(gd_type=\"Momentum\", delta=0.5), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_train_scaled, y_train)\n",
    "best_alpha = searcher.best_params_[\"alpha\"]\n",
    "print(\"Best alpha = %.4f\" % best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 714\n",
      "MSE = 0.11249514482946349\n",
      "R^2 = 0.7994593338373371\n"
     ]
    }
   ],
   "source": [
    "momentum_grad = LinReg(gd_type=\"Momentum\", delta=0.5, alpha=best_alpha).fit(X_train_scaled, y_train)\n",
    "y_pred = momentum_grad.predict(X_test_scaled)\n",
    "\n",
    "print(f'Number of iterations {len(momentum_grad.loss_history)}')\n",
    "print(f'MSE = {mean_squared_error(y_pred, y_test, squared=False)}')\n",
    "print(f'R^2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 1000\n",
      "MSE = 2.702014049699486\n",
      "R^2 = -114.69380396395604\n"
     ]
    }
   ],
   "source": [
    "adagrad_grad = LinReg(gd_type=\"Adagrad\", delta=0.5).fit(X_train_scaled, y_train) # Очень мало итераций цикла было, \n",
    "y_pred = adagrad_grad.predict(X_test_scaled)                                     # чтобы модель смогла хорошо обучиться\n",
    "\n",
    "print(f'Number of iterations {len(adagrad_grad.loss_history)}')\n",
    "print(f'MSE = {mean_squared_error(y_pred, y_test, squared=False)}')\n",
    "print(f'R^2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rIJNcxt_y-Qu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations 12022\n",
      "MSE = 1.4615730363959696\n",
      "R^2 = -32.851345838034305\n"
     ]
    }
   ],
   "source": [
    "adagrad_grad_25k_iters_omg = LinReg(gd_type=\"Adagrad\", delta=0.5, max_iter=25000).fit(X_train_scaled, y_train)\n",
    "y_pred = adagrad_grad_25k_iters_omg.predict(X_test_scaled)\n",
    "\n",
    "print(f'Number of iterations {len(adagrad_grad_25k_iters_omg.loss_history)}')\n",
    "print(f'MSE = {mean_squared_error(y_pred, y_test, squared=False)}')\n",
    "print(f'R^2 = {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Комментарии</span>\n",
    "```\n",
    "Ну в целом, ничего особо удивительного, написанная модель работает так же как и аналоги из sklearn и statsmodels, если судить по функции потерь и метрике r^2. Кроме того, я решил протестировать значение max_iter на самой долгой в плане обучения модели - Adargar, правда мы и смогли получить лишь 0.01 улучшения метрики r^2, что непозволительно мало, если брать в расчет количество операций, потраченных на такое незначительное улучшение, кроме того, данное улучшение модели оооочень медленно приближалось к гиперпараметру tolerance. Вот это было для меня неожиданностью. Мне не хватило 25000 операций, чтобы приблизится к значению 10^-6.\n",
    "По поводу значения aplha, я предполагал, что число должно находиться где-то в районе 0.1..1, потому что 1 - слишком много, а при приближении к 0 очень долго бы происходило обучение, но по факту вышло предполагаемое мной значение\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqYtVqv-y-Qu"
   },
   "source": [
    "#### 9. [1 балл] Постройте графики (там же) зависимости значения функции потерь от номера итерации для всех моделей (полного градиентого спуска, стохастического гс, Momentum и Adagrad). Сделайте выводы о скорости сходимости различных модификаций градиентного спуска.\n",
    "\n",
    "\n",
    "Не забывайте о том, как должен выглядеть *красивый* график!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Xbwhu8BSy-Qu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.collections import EventCollection\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# create random data\n",
    "xdata = np.random.random([2, 10])\n",
    "\n",
    "# split the data into two parts\n",
    "xdata1 = xdata[0, :]\n",
    "xdata2 = xdata[1, :]\n",
    "\n",
    "# sort the data so it makes clean curves\n",
    "xdata1.sort()\n",
    "xdata2.sort()\n",
    "\n",
    "# create some y data points\n",
    "ydata1 = xdata1 ** 2\n",
    "ydata2 = 1 - xdata2 ** 3\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(range((len(basic_grad.loss_history))), basic_grad.loss_history, label = \"Basic GD\")\n",
    "ax.plot(range(len(stochastic_grad.loss_history)), stochastic_grad.loss_history, label = \"Stochastic\")\n",
    "ax.plot(range(len(momentum_grad.loss_history)), momentum_grad.loss_history, label = \"Momentum_grad\")\n",
    "ax.plot(range(len(adagrad_grad.loss_history)), adagrad_grad.loss_history, label = \"Adagrad\")\n",
    "ax.plot(range(len(adagrad_grad_25k_iters_omg.loss_history)), adagrad_grad_25k_iters_omg.loss_history, \n",
    "        label = \"Adagrad with 25k max_iter and 10^-6 tolerance\")\n",
    "\n",
    "ax.set_title('Results')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Выводы</span>\n",
    "```\n",
    "Судя по графику, наибыстрейшим схождением обладает стохастический спуск, а худшим из представленных вышел Agarad. Все модели сходятся в целом одинаково, если не брать во внимание последний упомянутый. Думаю, это из-за очень маленьких шагов, которые выполняет эта модификация\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
