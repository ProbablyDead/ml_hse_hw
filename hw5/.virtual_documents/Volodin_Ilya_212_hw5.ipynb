








import sys

!{sys.executable} -m pip install --user seaborn ruff


from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold._t_sne import TSNE
from sklearn.cluster import DBSCAN


import pandas as pd
data = pd.read_csv('data_Mar_64.txt', header=None)
data.head()


data[0].unique().shape[0]





#Sixteen samples of leaf each of one-hundred plant species
data.shape





import numpy as np

random_state = 100
np.random.seed(random_state)

X, y_name = np.array(data.iloc[:, 1:]), data.iloc[:, 0]








y = LabelEncoder().fit_transform(y_name)





X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)





svm = SVC(kernel='linear',random_state=random_state)
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

print(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro'))


svm = SVC(kernel='linear', class_weight='balanced',random_state=random_state)
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

print(accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro'))


labels = ['negative','positive']

for i in confusion_matrix(y_test, y_pred):
    print(i)





PCA_X = PCA(n_components=2, random_state=1).fit_transform(X)





import matplotlib.pyplot as plt
%matplotlib inline

plt.scatter(PCA_X[y <= 15, 0], PCA_X[y <= 15, 1], c = y[y <= 15])





TSNE_X = TSNE(n_components=2, random_state=1).fit_transform(X)

plt.scatter(TSNE_X[y <= 15, 0], TSNE_X[y <= 15, 1], c = y[y <= 15])





cords_2_tsne = np.around(TSNE_X[2], decimals=2)
cords_2_tsne





cords_2_pca = np.around(PCA_X[2], decimals=2)
cords_2_pca











TSNE_X_train, TSNE_X_test, y_train, y_test = train_test_split(TSNE_X, y, test_size=0.3, random_state=1)

svm = SVC(kernel='linear', class_weight='balanced', random_state=random_state)
svm.fit(TSNE_X_train, y_train)
y_pred = svm.predict(TSNE_X_test)
accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='macro')











from IPython.display import clear_output
from sklearn.metrics import pairwise_distances_argmin

def plot_clust(X, centers, lables, ax): 
    ax.scatter(X[:,0], X[:,1], c=lables)
    ax.scatter(centers[:,0], centers[:,1], marker='>', color='red')

class MyKMeans():
    def __init__(self, n_clusters=3, n_iters=100, seed=None):
        self.n_clusters = n_clusters
        self.labels = None 
        self.centers = None 
        self.n_iters = n_iters
        self.seed = 0 if seed is None else seed
        np.random.seed(self.seed)        
    
    def update_centers(self, X):
        centers = np.empty((self.n_clusters, X.shape[1]))

        for i in range(self.n_clusters):
            if not self.labels[self.labels == i].size:
                centers[i] = (np.max(X, axis=0) - np.min(X, axis=0)) * np.random.rand(X.shape[1]) + np.min(X, axis=0) 
            else:
                centers[i] = np.mean(X[self.labels == i], axis=0)
        
        return centers
    
    def update_labels(self, X):
        labels = np.zeros((X.shape[0]))
        
        for i in range(labels.shape[0]):
            labels[i] = np.argmin(np.sum(np.square(X[i] - self.centers), axis=1), axis=0) #!
        
        return labels 

    def fit(self, X):
        self.centers = (np.max(X, axis=0) - np.min(X, axis=0)) * np.random.rand(self.n_clusters, X.shape[1]) + np.min(X, axis=0)
        self.labels = np.zeros((X.shape[0]))

        for it in range(self.n_iters):
            new_labels = self.update_labels(X)
            self.labels = new_labels

            new_centers = self.update_centers(X)
            if np.allclose(self.centers.flatten(), new_centers.flatten(), atol=1e-1):
                self.centers = new_centers
                self.labels = new_labels
                print('Converge by tolerance centers')

                fig, ax = plt.subplots(1,1)
                plot_clust(X, new_centers, new_labels, ax)
                return 0
      
            self.centers = new_centers

            fig, ax = plt.subplots(1,1)
            plot_clust(X, new_centers, new_labels, ax)
            plt.pause(0.3);
            clear_output(wait=True);
        

        return 1
    
    def predict(self, X):
        labels = np.array([np.argmin(np.linalg.norm(self.centers - X[i], axis=1)) for i in range(X.shape[0])])
        return labels





from sklearn import datasets
n_samples = 1000

noisy_blobs = datasets.make_blobs(n_samples=n_samples,
                             cluster_std=[1.0, 0.5, 0.5],
                             random_state=0)


X, y = noisy_blobs





kmeans_with_3s = MyKMeans(n_clusters=3, n_iters=3, seed=random_state)
tmp = kmeans_with_3s.fit(X)


print(X[0], kmeans_with_3s.predict(X)[0])





kmeans_with_3_100 = MyKMeans(n_clusters=3, n_iters=100, seed=random_state)
_ = kmeans_with_3_100.fit(X)


print(X[0], kmeans_with_3_100.predict(X)[0])





y[kmeans_with_3s.predict(X) != kmeans_with_3_100.predict(X)].size








rng = range(2, 50, 2)

sum_distances = np.arange(2, 50, 2)
for i in rng:
    i_means = MyKMeans(n_clusters=i, seed=random_state)
    i_means.fit(X)
    sum_distances[i // 2 - 1] = np.sum(np.square(X - i_means.centers[i_means.labels.astype(int)])) / i
plt.pause(0.3)
plt.plot(rng, sum_distances)


plt.plot(rng[:5], sum_distances[:5])
plt.plot([4, 4], [0, 1050], 'r--')


MyKMeans(n_clusters=4, n_iters=100, seed=random_state).fit(X)








pred = DBSCAN(eps=0.3).fit_predict(X)
pred[2]


plt.scatter(X[:, 0], X[:, 1], c=pred)





def tryModel(X, eps, min_samples):
    model = DBSCAN(eps=eps, min_samples=min_samples)
    plt.scatter(X[:, 0], X[:, 1], c=model.fit_predict(X))
    print("num of clusters:", np.unique(model.labels_[model.labels_ != -1]).size, "\n-1s: ", sum(model.labels_ == -1))


tryModel(X, 0.1, 2)


tryModel(X, 0.1, 7)


tryModel(X, 0.2, 2)


tryModel(X, 0.2, 7)


tryModel(X, 0.3, 2)


tryModel(X, 0.3, 7)


tryModel(X, 0.4, 2)


tryModel(X, 0.4, 7)


tryModel(X, 0.5, 2)


tryModel(X, 0.5, 7)



