


import sys  
!{sys.executable} -m pip install --user matplotlib pandas numpy graphviz pydotplus

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

plt.rcParams['figure.figsize'] = (11, 5)
%matplotlib inline

random_state = 42
np.random.seed(random_state)











def H(y):
    """
    Calculate impurity criterion
    
    Parameters
    ----------
    y : np.array
        array of objects target values in the node

    Returns
    -------
    H(R) : float
        Impurity in the node (measuread by variance)
    """
    if y.size == 0:
        return 0

    return np.var(y)


# Test the function
assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)
assert np.allclose(H(np.array([])), 0.0)

y = np.array([4, 2, 2, 2])
x = np.array([1, 0, 0, 1])





def Q(X, y, j, t):
    """
    Calculate cost function
    Parameters
    ----------
    X : ndarray
        array of objects in the node 
    y : ndarray
        array of target values in the node 
    j : int
        feature index (column in X)
    t : float
        threshold

    Returns
    -------
    Q : float
        Value of the cost function
    """ 
    chosen_column = X[:, j]
    R_l = y[chosen_column <= t]
    R_r = y[chosen_column > t]

    Q = (R_l.shape[0]/y.shape[0])*H(R_l) + (R_r.shape[0]/y.shape[0])*H(R_r)

    return Q





class Node(object):
    """
    Class for a decision tree node.
    
    Parameters
    ----------
    right : Node() or None
        Right child
    right : Node() or None
        Left child
    threshold: float
        
    column: int
        
    depth: int
        
    prediction: float
        prediction of the target value in the node 
        (average values calculated on a train dataset)
    is_terminal:bool
        indicates whether it is a terminal node (leaf) or not
    """    
    def __init__(self):        
        self.right = None
        self.left = None
        self.threshold = None
        self.column = None
        self.depth = None
        self.is_terminal = False
        self.prediction = None
        
    def __repr__(self):
        if self.is_terminal:
            node_desc = 'Pred: {:.2f}'.format(self.prediction)
        else:
            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \
            format(self.column, self.threshold, self.prediction)
        return node_desc


from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted

class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):
    """
    Class for a Decision Tree Regressor.

    Parameters
    ----------
    max_depth : int
        Max depth of a decision tree.
    min_samples_split : int
        Minimal number of samples (objects) in a node to make a split.
    """ 
    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1, criterion='gini'):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.criterion = 'gini'
            
    def best_split(self, X, y):
        """
        Find the best split in terms of Q of data in a given decision tree node. 
        Try all features and thresholds. 
        
        Parameters
        ----------
        X : ndarray, shape (n_objects, n_features)
            Objects in the parent node
        y : ndarray, shape (n_objects, )
            1D array with the object labels. 
            
        Returns
        -------
        best_split_column : int
            Index of the best split column
        best_threshold : float
            The best split condition.
        X_left : ndarray, shape (n_objects_l, n_features)
            Objects in the left child
        y_left : ndarray, shape (n_objects_l, )
            Objects labels in the left child. 
        X_right : ndarray, shape (n_objects_r, n_features)
            Objects in the right child
        y_right : ndarray, shape (n_objects_r, )
            Objects labels in the right child. 
        """
        
        # To store best split parameters
        best_split_column = None
        best_threshold = None
        # without splitting
        best_cost = H(y) 
        
        for column_id in range(X.shape[1]):
            column = X[:, column_id]
            for threshold in column:
                cost = Q(X, y, column_id, threshold)

                if cost < best_cost \
                and column[column <= threshold].shape[0] >= self.min_samples_leaf \
                and column[column > threshold].shape[0] >= self.min_samples_leaf:
                    best_cost = cost
                    best_split_column = column_id
                    best_threshold = threshold

        if best_split_column is None:
            return None, None, None, None, None, None

        column = X[:, best_split_column]
        X_left = X[column <= best_threshold, :]
        X_right = X[column > best_threshold, :]
        y_left = y[column <= best_threshold]
        y_right = y[column > best_threshold]
        
        return best_split_column, best_threshold, X_left, y_left, X_right, y_right
    
    def is_terminal(self, node, y):
        """
        Check terminality conditions based on `max_depth`, 
        `min_samples_split` parameters for a given node. 
        
        Parameters
        ----------
        node : Node, 
            
        y : ndarray, shape (n_objects, )
            Object labels. 
            
        Returns
        -------
        Is_termial : bool
            If True, node is terminal
        """
        if node.depth >= self.max_depth:    
            return True
        if len(y) < self.min_samples_split:   
            return True
        return False
        
    def grow_tree(self, node, X, y):
        """
        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:
         - check terminality conditions
         - find best split if node is not terminal
         - add child nodes to the node
         - call the function recursively for the added child nodes
        
        Parameters
        ----------
        node : Node() object
            Current node of the decision tree.
        X : ndarray, shape (n_objects, n_features)
            Objects 
        y : ndarray, shape (n_objects)
            Labels
        """
        
        if self.is_terminal(node, y):
            node.is_terminal = True
            node.prediction = np.mean(y)
            return
                
        column_split_id, threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)
            
        if column_split_id is None:
            node.is_terminal = True
            node.prediction = np.mean(y)
            return

        node.column = column_split_id
        node.threshold = threshold

        node.left = Node()
        node.left.depth = node.depth + 1
        node.left.predict = np.mean(y)

        node.right = Node()
        node.right.depth = node.depth + 1
        node.right.predict = np.mean(y)

        self.grow_tree(node.left, X_left, y_left)
        self.grow_tree(node.right, X_right, y_right)            

    def fit(self, X, y):
        """
        Fit the Decision Tree Regressor.
            
        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
            The input samples.
        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)
            The target values.
        Returns
        -------
        self : object
            Returns self.
        """
        X, y = check_X_y(X, y, accept_sparse=False)
        self.is_fitted_ = True
        
        self.n_features_in_ = X.shape[1] # Я вот это сюда добавил
        
        # Initialize the tree (root node)
        self.tree_ = Node()                             
        self.tree_.depth = 1                            
        self.tree_.prediction = np.mean(y)
        
        # Grow the tree
        self.grow_tree(self.tree_, X, y)
        return self        
    
    def get_prediction(self, node, x):
        """
        Get prediction for an object `x`
            - Return prediction of the `node` if it is terminal
            - Otherwise, recursively call the function to get 
            predictions of the proper child
        
        Parameters
        ----------
        node : Node() object
            Current node of the decision tree.
        x : ndarray, shape (n_features,)
            Array of feature values of one object.
        Returns
        -------
        y_pred : float
            Prediction for an object x
        """

        if node.is_terminal:
            return node.prediction

        y_pred = 0
        
        if x[node.column] <= node.threshold:
            y_pred = self.get_prediction(node.left, x)
        else:
            y_pred = self.get_prediction(node.right, x)
        
        return y_pred
        
    def predict(self, X):
        """ 
        Get prediction for each object in X
        
        Parameters
        ----------
        X : ndarray, shape (n_samples, n_features)
            The input samples.
        Returns
        -------
        y : ndarray, shape (n_samples,)
            Returns predictions.
        """
        # Check input and that `fit` had been called
        X = check_array(X, accept_sparse=False)
        check_is_fitted(self, 'is_fitted_')
        
        # Get predictions
        y_predicted = []
        for x in X:
            y_curr = self.get_prediction(self.tree_, x)
            y_predicted.append(y_curr)
        return np.array(y_predicted)


# check yourself
from sklearn.utils.estimator_checks import check_estimator

check_estimator(MyDecisionTreeRegressor())





from sklearn.tree import export_graphviz
import pickle, graphviz, pydotplus

dataset = pickle.load(open("./boston.pkl", "rb"))
X = pd.DataFrame(dataset.data, columns=dataset.feature_names)
y = dataset.target
X.head()


from sklearn.tree import export_graphviz
import os, graphviz,pydotplus

def tree_visualization(model, column, file_name="visualization"): 
    dot = export_graphviz(sk_model, filled=True, rounded=True, feature_names=X.columns, out_file=None)
    pydot_graph = pydotplus.graph_from_dot_data(dot)
    
    pydot_graph.write_png(file_name)
    img = plt.imread(file_name)
    plt.figure(figsize=(25, 25))
    plt.axis('off')
    plt.imshow(img)

def graph_show(pred, y, feature):
    pass

def describe_models(my_model, sk_model, max_depth, test_X, test_y):
    my_mea = mean_absolute_error(test_y, my_model.predict(test_X))
    sk_mea = mean_absolute_error(test_y, sk_model.predict(test_X))
    
    print(f"My model\n\tMEA score with {max_depth} max depth: {my_mea}")
    print(f"Sklearn model\n\tMEA score with {max_depth} max depth: {sk_mea}")
    print("\n")
    
    return (my_mea, sk_mea)


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)


from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error

max_depths = [1, 3, 5]
my_meas = []
sk_meas = []

for max_depth in max_depths:
    my_model = MyDecisionTreeRegressor(max_depth=max_depth)
    my_model.fit(X_train, y_train)
    
    sk_model = DecisionTreeRegressor(max_depth=max_depth)
    sk_model.fit(X_train, y_train)

    meas = describe_models(my_model, sk_model, max_depth, X_test, y_test)
    my_meas.append(meas[0])
    sk_meas.append(meas[1])

    tree_visualization(sk_model, X.columns, file_name=f"4_md_{max_depth}")

plt.style.use('_mpl-gallery')

fig, ax = plt.subplots()
ax.plot(max_depths, my_meas, linewidth=2.0, label="My")
ax.plot(max_depths, sk_meas, linewidth=2.0, label="Sk")
plt.xlabel("Max depth")
plt.ylabel("MEA")
plt.legend()





from matplotlib.pyplot import figure

plt.plot(X_train.LSTAT, y_train, 'o', label="train set")
plt.plot(X_test.LSTAT, my_model.predict(X_test), 'ro', label="predictions")
plt.xlabel("LSTAT")
plt.ylabel("Target")
plt.legend()
plt.figure(figsize=(80, 80))








parameters = { "max_depth": [1, 3, 5, 10], "min_samples_leaf": [300, 200, 100, 50, 10, 1] }


from sklearn.model_selection import GridSearchCV

searcher = GridSearchCV(MyDecisionTreeRegressor(), parameters, verbose=2, cv=5)
searcher.fit(X_train, y_train)

best_hyperparameters = searcher.best_params_


my_model = MyDecisionTreeRegressor(max_depth=best_hyperparameters["max_depth"], min_samples_leaf=best_hyperparameters["min_samples_leaf"])
my_model.fit(X_train, y_train)

mean_absolute_error(y_test, my_model.predict(X_test))








import warnings

def get_bias_variance(estimator, x, y, n_iter):
    """ 
    Calculate bias and variance of the `estimator`. 
    Using a given dataset and bootstrap with `n_iter` samples. 

    Parameters
    ----------
    x : ndarray, shape (n_samples, n_features)
        The input samples.
    y : ndarray, shape (n_samples, n_features)
        The input samples.
    n_iter: int
        Number of samples in 
    Returns
    -------
    bias2 : float, 
        Estiamted squared bias
    variance : float, 
        Estiamted variance
    """   
    
    preds = np.empty((n_iter, x.shape[0]))
    preds[:] = np.nan

    for i in range(n_iter):
        idx = np.random.choice(x.shape[0], size=x.shape[0])
        oob_idx = np.setdiff1d(np.arange(x.shape[0]), idx)
        pred = estimator.fit(x[idx, :], y[idx]).predict(x[oob_idx])
        
        preds[i, oob_idx] = pred

    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=RuntimeWarning)
        bias2 = np.nanmean((np.nanmean(preds, axis=0) - y) ** 2)
        variance = np.nanmean(np.nanvar(preds, axis=0))

    return bias2, variance


# Test
estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)

get_bias_variance(estimator, X_train.values, y_train, 10)





my_bias2es = []
my_variances = []
sk_bias2es = []
sk_variances = []
params = list(range(2, 312, 10))

for min_samples_split in params:
    estimator = MyDecisionTreeRegressor(max_depth=best_hyperparameters["max_depth"], 
                                        min_samples_leaf=best_hyperparameters["min_samples_leaf"], min_samples_split=min_samples_split)
    bias2, variance = get_bias_variance(estimator, X_train.values, y_train, 10)
    my_bias2es.append(bias2)
    my_variances.append(variance)

    estimator = DecisionTreeRegressor(max_depth=best_hyperparameters["max_depth"], 
                                        min_samples_leaf=best_hyperparameters["min_samples_leaf"], min_samples_split=min_samples_split)
    bias2, variance = get_bias_variance(estimator, X_train.values, y_train, 10)
    sk_bias2es.append(bias2)
    sk_variances.append(variance)


fig, axs = plt.subplots(1, 2)

axs[0].set_title("My decision tree")
axs[0].plot(params, my_bias2es)
axs[0].plot(params, my_variances)
axs[0].text(6, 20, "Bias2")
axs[0].text(6, 9, "Variance")

axs[1].set_title("Sckit learn decision tree")
axs[1].plot(params, sk_bias2es)
axs[1].plot(params, sk_variances)
axs[1].text(6, 20, "Bias2")
axs[1].text(6, 9, "Variance")

fig.set_size_inches(18.5, 5)








from sklearn.ensemble import BaggingRegressor

estimator = BaggingRegressor(estimator=MyDecisionTreeRegressor(max_depth=8, min_samples_split=15), random_state=random_state)

get_bias_variance(estimator, np.array(X_train), y_train, 10)











from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('Billionaires Statistics Dataset.csv')

le = LabelEncoder()
y = le.fit_transform(df['selfMade'])
X = df.drop('selfMade', axis=1)
X.head(5)


X.info()


X.columns





X = X.drop(["personName", "source", "organization", "birthDate", "lastName", "firstName", "title", "date", "state",
        "residenceStateRegion", "birthYear", "birthMonth", "birthDay", "latitude_country", "longitude_country", "city", "countryOfCitizenship"], axis=1)
X.info()





X.gdp_country.loc[X.gdp_country.notnull()] = X.gdp_country[X.gdp_country.notnull()].apply(lambda x: int(x.replace("$", "").replace(",", "").replace(" ", "")))
X.head()


X = X.dropna(subset=['country'])


non_str_columns = ["rank", "finalWorth", "age", "cpi_country", "cpi_change_country", "gdp_country", "gross_tertiary_education_enrollment", "gross_primary_education_enrollment_country", "life_expectancy_country", "tax_revenue_country_country", "total_tax_rate_country", "population_country"]

X[non_str_columns] = X[non_str_columns].fillna(X[non_str_columns].mean())


X.info()


X.head()


from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)
categorical_columns = X.drop(non_str_columns).columns

# define column_transformer 

# Transform the data
X_train = column_transformer.fit_transform(X_train)
X_test = column_transformer.transform(X_test)





# YOUR CODE HERE








# YOUR CODE HERE








# YOUR CODE HERE








# YOUR CODE HERE



